2015-01-14 19:28:15,024 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = barbie/127.0.1.1
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.6.0
STARTUP_MSG:   classpath = /usr/local/hadoop-2.6.0/etc/hadoop:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/commons-collections-3.2.1.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/hadoop-annotations-2.6.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/jasper-runtime-5.5.23.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/curator-client-2.6.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/curator-framework-2.6.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/jsr305-1.3.9.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/curator-recipes-2.6.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/htrace-core-3.0.4.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/commons-el-1.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/jasper-compiler-5.5.23.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/hadoop-auth-2.6.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/slf4j-api-1.7.5.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/hadoop-common-2.6.0-tests.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/hadoop-common-2.6.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/hadoop-nfs-2.6.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/hdfs:/usr/local/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop-2.6.0/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.6.0/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop-2.6.0/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.6.0/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.6.0/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop-2.6.0/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.0/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.0/share/hadoop/hdfs/lib/jasper-runtime-5.5.23.jar:/usr/local/hadoop-2.6.0/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop-2.6.0/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.6.0/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.6.0/share/hadoop/hdfs/lib/jsr305-1.3.9.jar:/usr/local/hadoop-2.6.0/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop-2.6.0/share/hadoop/hdfs/lib/htrace-core-3.0.4.jar:/usr/local/hadoop-2.6.0/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-el-1.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.0/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.6.0/share/hadoop/hdfs/lib/jsp-api-2.1.jar:/usr/local/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-2.6.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-2.6.0-tests.jar:/usr/local/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-nfs-2.6.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/lib/commons-httpclient-3.1.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/lib/jsr305-1.3.9.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/lib/jline-0.9.94.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-registry-2.6.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.6.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-tests-2.6.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.6.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-client-2.6.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.6.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-common-2.6.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.6.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-common-2.6.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.6.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-api-2.6.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.6.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/mapreduce/lib/hadoop-annotations-2.6.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop-2.6.0/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.0/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop-2.6.0/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop-2.6.0/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.0/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.0/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop-2.6.0/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop-2.6.0/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop-2.6.0/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop-2.6.0/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.0/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop-2.6.0/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.6.0/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.0/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.0/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.6.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.6.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.6.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.6.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.6.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.0-tests.jar:/usr/local/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.6.0.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://github.com/apache/hadoop -r e3496499ecb8d220fba99dc5ed4c99c8f9e33bb1; compiled by 'doopy' on 2015-01-14T23:00Z
STARTUP_MSG:   java = 1.7.0_65
************************************************************/
2015-01-14 19:28:15,088 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-01-14 19:28:22,190 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-01-14 19:28:23,541 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-01-14 19:28:23,542 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2015-01-14 19:28:23,601 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is barbie
2015-01-14 19:28:23,740 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2015-01-14 19:28:24,196 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2015-01-14 19:28:24,223 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2015-01-14 19:28:24,223 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2015-01-14 19:28:25,062 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-01-14 19:28:25,129 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2015-01-14 19:28:25,293 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-01-14 19:28:25,358 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2015-01-14 19:28:25,358 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-01-14 19:28:25,358 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-01-14 19:28:25,613 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.datanode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2015-01-14 19:28:25,664 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50075
2015-01-14 19:28:25,664 INFO org.mortbay.log: jetty-6.1.26
2015-01-14 19:28:29,921 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50075
2015-01-14 19:28:30,055 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = doopy
2015-01-14 19:28:30,055 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2015-01-14 19:28:30,522 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-01-14 19:28:30,692 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2015-01-14 19:28:31,376 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2015-01-14 19:28:31,500 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2015-01-14 19:28:31,708 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2015-01-14 19:28:31,776 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:9000 starting to offer service
2015-01-14 19:28:31,816 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-01-14 19:28:31,818 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2015-01-14 19:28:33,039 INFO org.apache.hadoop.hdfs.server.common.Storage: DataNode version: -56 and NameNode layout version: -60
2015-01-14 19:28:33,106 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /tmp/hadoop-doopy/dfs/data/in_use.lock acquired by nodename 2574@barbie
2015-01-14 19:28:33,109 INFO org.apache.hadoop.hdfs.server.common.Storage: Storage directory /tmp/hadoop-doopy/dfs/data is not formatted for BP-1611003397-127.0.1.1-1421285133099
2015-01-14 19:28:33,109 INFO org.apache.hadoop.hdfs.server.common.Storage: Formatting ...
2015-01-14 19:28:33,734 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-1611003397-127.0.1.1-1421285133099
2015-01-14 19:28:33,734 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled
2015-01-14 19:28:33,736 INFO org.apache.hadoop.hdfs.server.common.Storage: Storage directory /tmp/hadoop-doopy/dfs/data/current/BP-1611003397-127.0.1.1-1421285133099 is not formatted.
2015-01-14 19:28:33,744 INFO org.apache.hadoop.hdfs.server.common.Storage: Formatting ...
2015-01-14 19:28:33,744 INFO org.apache.hadoop.hdfs.server.common.Storage: Formatting block pool BP-1611003397-127.0.1.1-1421285133099 directory /tmp/hadoop-doopy/dfs/data/current/BP-1611003397-127.0.1.1-1421285133099/current
2015-01-14 19:28:33,774 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.
2015-01-14 19:28:33,808 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=131792437;bpid=BP-1611003397-127.0.1.1-1421285133099;lv=-56;nsInfo=lv=-60;cid=CID-a0ed2779-c968-4c23-862a-f2f91301194e;nsid=131792437;c=0;bpid=BP-1611003397-127.0.1.1-1421285133099;dnuuid=null
2015-01-14 19:28:33,841 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Generated and persisted new Datanode UUID 47972a9c-c7e0-400a-ad2b-3a4f63bb22be
2015-01-14 19:28:34,087 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: /tmp/hadoop-doopy/dfs/data/current
2015-01-14 19:28:34,098 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /tmp/hadoop-doopy/dfs/data/current, StorageType: DISK
2015-01-14 19:28:34,145 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2015-01-14 19:28:34,166 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1421289078166 with interval 21600000
2015-01-14 19:28:34,172 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-1611003397-127.0.1.1-1421285133099
2015-01-14 19:28:34,180 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-1611003397-127.0.1.1-1421285133099 on volume /tmp/hadoop-doopy/dfs/data/current...
2015-01-14 19:28:34,242 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-1611003397-127.0.1.1-1421285133099 on /tmp/hadoop-doopy/dfs/data/current: 62ms
2015-01-14 19:28:34,242 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-1611003397-127.0.1.1-1421285133099: 70ms
2015-01-14 19:28:34,243 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-1611003397-127.0.1.1-1421285133099 on volume /tmp/hadoop-doopy/dfs/data/current...
2015-01-14 19:28:34,244 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-1611003397-127.0.1.1-1421285133099 on volume /tmp/hadoop-doopy/dfs/data/current: 0ms
2015-01-14 19:28:34,244 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 2ms
2015-01-14 19:28:34,258 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-1611003397-127.0.1.1-1421285133099 (Datanode Uuid null) service to localhost/127.0.0.1:9000 beginning handshake with NN
2015-01-14 19:28:34,479 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-1611003397-127.0.1.1-1421285133099 (Datanode Uuid null) service to localhost/127.0.0.1:9000 successfully registered with NN
2015-01-14 19:28:34,479 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode localhost/127.0.0.1:9000 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2015-01-14 19:28:35,018 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Namenode Block pool BP-1611003397-127.0.1.1-1421285133099 (Datanode Uuid 47972a9c-c7e0-400a-ad2b-3a4f63bb22be) service to localhost/127.0.0.1:9000 trying to claim ACTIVE state with txid=1
2015-01-14 19:28:35,018 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode Block pool BP-1611003397-127.0.1.1-1421285133099 (Datanode Uuid 47972a9c-c7e0-400a-ad2b-3a4f63bb22be) service to localhost/127.0.0.1:9000
2015-01-14 19:28:35,258 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Sent 1 blockreports 0 blocks total. Took 3 msec to generate and 236 msecs for RPC and NN processing.  Got back commands org.apache.hadoop.hdfs.server.protocol.FinalizeCommand@34f7ea
2015-01-14 19:28:35,258 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1611003397-127.0.1.1-1421285133099
2015-01-14 19:28:35,279 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlockMap
2015-01-14 19:28:35,279 INFO org.apache.hadoop.util.GSet: VM type       = 32-bit
2015-01-14 19:28:35,286 INFO org.apache.hadoop.util.GSet: 0.5% max memory 966.7 MB = 4.8 MB
2015-01-14 19:28:35,286 INFO org.apache.hadoop.util.GSet: capacity      = 2^20 = 1048576 entries
2015-01-14 19:28:35,297 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Periodic Block Verification Scanner initialized with interval 504 hours for block pool BP-1611003397-127.0.1.1-1421285133099
2015-01-14 19:28:35,328 INFO org.apache.hadoop.hdfs.server.datanode.DataBlockScanner: Added bpid=BP-1611003397-127.0.1.1-1421285133099 to blockPoolScannerMap, new size=1
2015-01-14 19:29:39,163 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1611003397-127.0.1.1-1421285133099:blk_1073741825_1001 src: /127.0.0.1:59423 dest: /127.0.0.1:50010
2015-01-14 19:29:39,447 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:59423, dest: /127.0.0.1:50010, bytes: 4436, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_189475487_1, offset: 0, srvID: 47972a9c-c7e0-400a-ad2b-3a4f63bb22be, blockid: BP-1611003397-127.0.1.1-1421285133099:blk_1073741825_1001, duration: 203684915
2015-01-14 19:29:39,452 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1611003397-127.0.1.1-1421285133099:blk_1073741825_1001, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-01-14 19:29:40,051 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1611003397-127.0.1.1-1421285133099:blk_1073741826_1002 src: /127.0.0.1:59424 dest: /127.0.0.1:50010
2015-01-14 19:29:40,062 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:59424, dest: /127.0.0.1:50010, bytes: 1335, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_189475487_1, offset: 0, srvID: 47972a9c-c7e0-400a-ad2b-3a4f63bb22be, blockid: BP-1611003397-127.0.1.1-1421285133099:blk_1073741826_1002, duration: 3463562
2015-01-14 19:29:40,066 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1611003397-127.0.1.1-1421285133099:blk_1073741826_1002, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-01-14 19:29:40,142 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1611003397-127.0.1.1-1421285133099:blk_1073741827_1003 src: /127.0.0.1:59425 dest: /127.0.0.1:50010
2015-01-14 19:29:40,152 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:59425, dest: /127.0.0.1:50010, bytes: 318, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_189475487_1, offset: 0, srvID: 47972a9c-c7e0-400a-ad2b-3a4f63bb22be, blockid: BP-1611003397-127.0.1.1-1421285133099:blk_1073741827_1003, duration: 1865729
2015-01-14 19:29:40,155 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1611003397-127.0.1.1-1421285133099:blk_1073741827_1003, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-01-14 19:29:40,209 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1611003397-127.0.1.1-1421285133099:blk_1073741828_1004 src: /127.0.0.1:59426 dest: /127.0.0.1:50010
2015-01-14 19:29:40,219 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:59426, dest: /127.0.0.1:50010, bytes: 866, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_189475487_1, offset: 0, srvID: 47972a9c-c7e0-400a-ad2b-3a4f63bb22be, blockid: BP-1611003397-127.0.1.1-1421285133099:blk_1073741828_1004, duration: 4314881
2015-01-14 19:29:40,222 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1611003397-127.0.1.1-1421285133099:blk_1073741828_1004, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-01-14 19:29:40,301 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1611003397-127.0.1.1-1421285133099:blk_1073741829_1005 src: /127.0.0.1:59427 dest: /127.0.0.1:50010
2015-01-14 19:29:40,310 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:59427, dest: /127.0.0.1:50010, bytes: 3670, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_189475487_1, offset: 0, srvID: 47972a9c-c7e0-400a-ad2b-3a4f63bb22be, blockid: BP-1611003397-127.0.1.1-1421285133099:blk_1073741829_1005, duration: 3518972
2015-01-14 19:29:40,314 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1611003397-127.0.1.1-1421285133099:blk_1073741829_1005, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-01-14 19:29:40,359 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1611003397-127.0.1.1-1421285133099:blk_1073741830_1006 src: /127.0.0.1:59428 dest: /127.0.0.1:50010
2015-01-14 19:29:40,368 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:59428, dest: /127.0.0.1:50010, bytes: 4244, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_189475487_1, offset: 0, srvID: 47972a9c-c7e0-400a-ad2b-3a4f63bb22be, blockid: BP-1611003397-127.0.1.1-1421285133099:blk_1073741830_1006, duration: 3352424
2015-01-14 19:29:40,373 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1611003397-127.0.1.1-1421285133099:blk_1073741830_1006, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-01-14 19:29:40,417 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1611003397-127.0.1.1-1421285133099:blk_1073741831_1007 src: /127.0.0.1:59429 dest: /127.0.0.1:50010
2015-01-14 19:29:40,431 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:59429, dest: /127.0.0.1:50010, bytes: 2490, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_189475487_1, offset: 0, srvID: 47972a9c-c7e0-400a-ad2b-3a4f63bb22be, blockid: BP-1611003397-127.0.1.1-1421285133099:blk_1073741831_1007, duration: 3994673
2015-01-14 19:29:40,434 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1611003397-127.0.1.1-1421285133099:blk_1073741831_1007, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-01-14 19:29:40,484 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1611003397-127.0.1.1-1421285133099:blk_1073741832_1008 src: /127.0.0.1:59430 dest: /127.0.0.1:50010
2015-01-14 19:29:40,499 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:59430, dest: /127.0.0.1:50010, bytes: 2598, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_189475487_1, offset: 0, srvID: 47972a9c-c7e0-400a-ad2b-3a4f63bb22be, blockid: BP-1611003397-127.0.1.1-1421285133099:blk_1073741832_1008, duration: 8520422
2015-01-14 19:29:40,503 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1611003397-127.0.1.1-1421285133099:blk_1073741832_1008, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-01-14 19:29:40,543 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1611003397-127.0.1.1-1421285133099:blk_1073741833_1009 src: /127.0.0.1:59431 dest: /127.0.0.1:50010
2015-01-14 19:29:40,557 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:59431, dest: /127.0.0.1:50010, bytes: 9683, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_189475487_1, offset: 0, srvID: 47972a9c-c7e0-400a-ad2b-3a4f63bb22be, blockid: BP-1611003397-127.0.1.1-1421285133099:blk_1073741833_1009, duration: 3905074
2015-01-14 19:29:40,561 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1611003397-127.0.1.1-1421285133099:blk_1073741833_1009, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-01-14 19:29:40,600 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1611003397-127.0.1.1-1421285133099:blk_1073741834_1010 src: /127.0.0.1:59432 dest: /127.0.0.1:50010
2015-01-14 19:29:40,609 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:59432, dest: /127.0.0.1:50010, bytes: 849, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_189475487_1, offset: 0, srvID: 47972a9c-c7e0-400a-ad2b-3a4f63bb22be, blockid: BP-1611003397-127.0.1.1-1421285133099:blk_1073741834_1010, duration: 3407130
2015-01-14 19:29:40,613 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1611003397-127.0.1.1-1421285133099:blk_1073741834_1010, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-01-14 19:29:40,702 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1611003397-127.0.1.1-1421285133099:blk_1073741835_1011 src: /127.0.0.1:59433 dest: /127.0.0.1:50010
2015-01-14 19:29:40,711 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:59433, dest: /127.0.0.1:50010, bytes: 1449, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_189475487_1, offset: 0, srvID: 47972a9c-c7e0-400a-ad2b-3a4f63bb22be, blockid: BP-1611003397-127.0.1.1-1421285133099:blk_1073741835_1011, duration: 3381634
2015-01-14 19:29:40,715 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1611003397-127.0.1.1-1421285133099:blk_1073741835_1011, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-01-14 19:29:40,792 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1611003397-127.0.1.1-1421285133099:blk_1073741836_1012 src: /127.0.0.1:59434 dest: /127.0.0.1:50010
2015-01-14 19:29:40,800 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:59434, dest: /127.0.0.1:50010, bytes: 1657, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_189475487_1, offset: 0, srvID: 47972a9c-c7e0-400a-ad2b-3a4f63bb22be, blockid: BP-1611003397-127.0.1.1-1421285133099:blk_1073741836_1012, duration: 3119493
2015-01-14 19:29:40,804 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1611003397-127.0.1.1-1421285133099:blk_1073741836_1012, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-01-14 19:29:40,851 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1611003397-127.0.1.1-1421285133099:blk_1073741837_1013 src: /127.0.0.1:59435 dest: /127.0.0.1:50010
2015-01-14 19:29:40,860 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:59435, dest: /127.0.0.1:50010, bytes: 21, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_189475487_1, offset: 0, srvID: 47972a9c-c7e0-400a-ad2b-3a4f63bb22be, blockid: BP-1611003397-127.0.1.1-1421285133099:blk_1073741837_1013, duration: 3316972
2015-01-14 19:29:40,864 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1611003397-127.0.1.1-1421285133099:blk_1073741837_1013, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-01-14 19:29:40,917 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1611003397-127.0.1.1-1421285133099:blk_1073741838_1014 src: /127.0.0.1:59436 dest: /127.0.0.1:50010
2015-01-14 19:29:40,926 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:59436, dest: /127.0.0.1:50010, bytes: 620, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_189475487_1, offset: 0, srvID: 47972a9c-c7e0-400a-ad2b-3a4f63bb22be, blockid: BP-1611003397-127.0.1.1-1421285133099:blk_1073741838_1014, duration: 3584850
2015-01-14 19:29:40,930 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1611003397-127.0.1.1-1421285133099:blk_1073741838_1014, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-01-14 19:29:40,975 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1611003397-127.0.1.1-1421285133099:blk_1073741839_1015 src: /127.0.0.1:59437 dest: /127.0.0.1:50010
2015-01-14 19:29:40,984 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:59437, dest: /127.0.0.1:50010, bytes: 3523, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_189475487_1, offset: 0, srvID: 47972a9c-c7e0-400a-ad2b-3a4f63bb22be, blockid: BP-1611003397-127.0.1.1-1421285133099:blk_1073741839_1015, duration: 3097168
2015-01-14 19:29:40,989 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1611003397-127.0.1.1-1421285133099:blk_1073741839_1015, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-01-14 19:29:41,076 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1611003397-127.0.1.1-1421285133099:blk_1073741840_1016 src: /127.0.0.1:59438 dest: /127.0.0.1:50010
2015-01-14 19:29:41,085 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:59438, dest: /127.0.0.1:50010, bytes: 1325, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_189475487_1, offset: 0, srvID: 47972a9c-c7e0-400a-ad2b-3a4f63bb22be, blockid: BP-1611003397-127.0.1.1-1421285133099:blk_1073741840_1016, duration: 3187602
2015-01-14 19:29:41,089 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1611003397-127.0.1.1-1421285133099:blk_1073741840_1016, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-01-14 19:29:41,142 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1611003397-127.0.1.1-1421285133099:blk_1073741841_1017 src: /127.0.0.1:59439 dest: /127.0.0.1:50010
2015-01-14 19:29:41,150 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:59439, dest: /127.0.0.1:50010, bytes: 1631, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_189475487_1, offset: 0, srvID: 47972a9c-c7e0-400a-ad2b-3a4f63bb22be, blockid: BP-1611003397-127.0.1.1-1421285133099:blk_1073741841_1017, duration: 3011507
2015-01-14 19:29:41,154 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1611003397-127.0.1.1-1421285133099:blk_1073741841_1017, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-01-14 19:29:41,192 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1611003397-127.0.1.1-1421285133099:blk_1073741842_1018 src: /127.0.0.1:59440 dest: /127.0.0.1:50010
2015-01-14 19:29:41,202 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:59440, dest: /127.0.0.1:50010, bytes: 5511, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_189475487_1, offset: 0, srvID: 47972a9c-c7e0-400a-ad2b-3a4f63bb22be, blockid: BP-1611003397-127.0.1.1-1421285133099:blk_1073741842_1018, duration: 4381531
2015-01-14 19:29:41,205 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1611003397-127.0.1.1-1421285133099:blk_1073741842_1018, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-01-14 19:29:41,251 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1611003397-127.0.1.1-1421285133099:blk_1073741843_1019 src: /127.0.0.1:59441 dest: /127.0.0.1:50010
2015-01-14 19:29:41,262 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:59441, dest: /127.0.0.1:50010, bytes: 11291, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_189475487_1, offset: 0, srvID: 47972a9c-c7e0-400a-ad2b-3a4f63bb22be, blockid: BP-1611003397-127.0.1.1-1421285133099:blk_1073741843_1019, duration: 4526547
2015-01-14 19:29:41,265 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1611003397-127.0.1.1-1421285133099:blk_1073741843_1019, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-01-14 19:29:41,342 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1611003397-127.0.1.1-1421285133099:blk_1073741844_1020 src: /127.0.0.1:59442 dest: /127.0.0.1:50010
2015-01-14 19:29:41,351 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:59442, dest: /127.0.0.1:50010, bytes: 938, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_189475487_1, offset: 0, srvID: 47972a9c-c7e0-400a-ad2b-3a4f63bb22be, blockid: BP-1611003397-127.0.1.1-1421285133099:blk_1073741844_1020, duration: 3864537
2015-01-14 19:29:41,355 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1611003397-127.0.1.1-1421285133099:blk_1073741844_1020, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-01-14 19:29:41,400 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1611003397-127.0.1.1-1421285133099:blk_1073741845_1021 src: /127.0.0.1:59443 dest: /127.0.0.1:50010
2015-01-14 19:29:41,408 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:59443, dest: /127.0.0.1:50010, bytes: 1383, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_189475487_1, offset: 0, srvID: 47972a9c-c7e0-400a-ad2b-3a4f63bb22be, blockid: BP-1611003397-127.0.1.1-1421285133099:blk_1073741845_1021, duration: 2668914
2015-01-14 19:29:41,413 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1611003397-127.0.1.1-1421285133099:blk_1073741845_1021, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-01-14 19:29:41,475 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1611003397-127.0.1.1-1421285133099:blk_1073741846_1022 src: /127.0.0.1:59444 dest: /127.0.0.1:50010
2015-01-14 19:29:41,485 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:59444, dest: /127.0.0.1:50010, bytes: 4113, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_189475487_1, offset: 0, srvID: 47972a9c-c7e0-400a-ad2b-3a4f63bb22be, blockid: BP-1611003397-127.0.1.1-1421285133099:blk_1073741846_1022, duration: 1851258
2015-01-14 19:29:41,488 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1611003397-127.0.1.1-1421285133099:blk_1073741846_1022, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-01-14 19:29:41,572 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1611003397-127.0.1.1-1421285133099:blk_1073741847_1023 src: /127.0.0.1:59445 dest: /127.0.0.1:50010
2015-01-14 19:29:41,580 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:59445, dest: /127.0.0.1:50010, bytes: 758, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_189475487_1, offset: 0, srvID: 47972a9c-c7e0-400a-ad2b-3a4f63bb22be, blockid: BP-1611003397-127.0.1.1-1421285133099:blk_1073741847_1023, duration: 2593322
2015-01-14 19:29:41,583 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1611003397-127.0.1.1-1421285133099:blk_1073741847_1023, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-01-14 19:29:41,625 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1611003397-127.0.1.1-1421285133099:blk_1073741848_1024 src: /127.0.0.1:59446 dest: /127.0.0.1:50010
2015-01-14 19:29:41,633 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:59446, dest: /127.0.0.1:50010, bytes: 10, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_189475487_1, offset: 0, srvID: 47972a9c-c7e0-400a-ad2b-3a4f63bb22be, blockid: BP-1611003397-127.0.1.1-1421285133099:blk_1073741848_1024, duration: 2782873
2015-01-14 19:29:41,640 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1611003397-127.0.1.1-1421285133099:blk_1073741848_1024, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-01-14 19:29:41,708 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1611003397-127.0.1.1-1421285133099:blk_1073741849_1025 src: /127.0.0.1:59447 dest: /127.0.0.1:50010
2015-01-14 19:29:41,718 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:59447, dest: /127.0.0.1:50010, bytes: 2316, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_189475487_1, offset: 0, srvID: 47972a9c-c7e0-400a-ad2b-3a4f63bb22be, blockid: BP-1611003397-127.0.1.1-1421285133099:blk_1073741849_1025, duration: 3933157
2015-01-14 19:29:41,722 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1611003397-127.0.1.1-1421285133099:blk_1073741849_1025, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-01-14 19:29:41,767 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1611003397-127.0.1.1-1421285133099:blk_1073741850_1026 src: /127.0.0.1:59448 dest: /127.0.0.1:50010
2015-01-14 19:29:41,775 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:59448, dest: /127.0.0.1:50010, bytes: 2268, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_189475487_1, offset: 0, srvID: 47972a9c-c7e0-400a-ad2b-3a4f63bb22be, blockid: BP-1611003397-127.0.1.1-1421285133099:blk_1073741850_1026, duration: 1580232
2015-01-14 19:29:41,778 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1611003397-127.0.1.1-1421285133099:blk_1073741850_1026, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-01-14 19:29:41,817 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1611003397-127.0.1.1-1421285133099:blk_1073741851_1027 src: /127.0.0.1:59449 dest: /127.0.0.1:50010
2015-01-14 19:29:41,826 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:59449, dest: /127.0.0.1:50010, bytes: 2237, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_189475487_1, offset: 0, srvID: 47972a9c-c7e0-400a-ad2b-3a4f63bb22be, blockid: BP-1611003397-127.0.1.1-1421285133099:blk_1073741851_1027, duration: 3159801
2015-01-14 19:29:41,829 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1611003397-127.0.1.1-1421285133099:blk_1073741851_1027, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-01-14 19:29:41,900 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1611003397-127.0.1.1-1421285133099:blk_1073741852_1028 src: /127.0.0.1:59450 dest: /127.0.0.1:50010
2015-01-14 19:29:41,908 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:59450, dest: /127.0.0.1:50010, bytes: 4567, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_189475487_1, offset: 0, srvID: 47972a9c-c7e0-400a-ad2b-3a4f63bb22be, blockid: BP-1611003397-127.0.1.1-1421285133099:blk_1073741852_1028, duration: 3170500
2015-01-14 19:29:41,912 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1611003397-127.0.1.1-1421285133099:blk_1073741852_1028, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-01-14 19:29:41,950 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1611003397-127.0.1.1-1421285133099:blk_1073741853_1029 src: /127.0.0.1:59451 dest: /127.0.0.1:50010
2015-01-14 19:29:41,959 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:59451, dest: /127.0.0.1:50010, bytes: 690, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_189475487_1, offset: 0, srvID: 47972a9c-c7e0-400a-ad2b-3a4f63bb22be, blockid: BP-1611003397-127.0.1.1-1421285133099:blk_1073741853_1029, duration: 1576421
2015-01-14 19:29:41,962 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1611003397-127.0.1.1-1421285133099:blk_1073741853_1029, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-01-14 19:29:49,265 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1611003397-127.0.1.1-1421285133099:blk_1073741843_1019
2015-01-14 19:29:49,269 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1611003397-127.0.1.1-1421285133099:blk_1073741829_1005
2015-01-14 19:29:49,271 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1611003397-127.0.1.1-1421285133099:blk_1073741842_1018
2015-01-14 19:29:49,273 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1611003397-127.0.1.1-1421285133099:blk_1073741845_1021
2015-01-14 19:29:49,274 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1611003397-127.0.1.1-1421285133099:blk_1073741830_1006
2015-01-14 19:29:49,276 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1611003397-127.0.1.1-1421285133099:blk_1073741840_1016
2015-01-14 19:30:39,831 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1611003397-127.0.1.1-1421285133099:blk_1073741854_1030 src: /127.0.0.1:59455 dest: /127.0.0.1:50010
2015-01-14 19:30:39,914 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:59455, dest: /127.0.0.1:50010, bytes: 437, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1954089522_1, offset: 0, srvID: 47972a9c-c7e0-400a-ad2b-3a4f63bb22be, blockid: BP-1611003397-127.0.1.1-1421285133099:blk_1073741854_1030, duration: 61933458
2015-01-14 19:30:39,914 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1611003397-127.0.1.1-1421285133099:blk_1073741854_1030, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-01-14 19:30:41,629 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1611003397-127.0.1.1-1421285133099:blk_1073741855_1031 src: /127.0.0.1:59456 dest: /127.0.0.1:50010
2015-01-14 19:30:41,639 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:59456, dest: /127.0.0.1:50010, bytes: 197, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1954089522_1, offset: 0, srvID: 47972a9c-c7e0-400a-ad2b-3a4f63bb22be, blockid: BP-1611003397-127.0.1.1-1421285133099:blk_1073741855_1031, duration: 4586150
2015-01-14 19:30:41,643 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1611003397-127.0.1.1-1421285133099:blk_1073741855_1031, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-01-14 19:30:46,507 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741854_1030 file /tmp/hadoop-doopy/dfs/data/current/BP-1611003397-127.0.1.1-1421285133099/current/finalized/subdir0/subdir0/blk_1073741854 for deletion
2015-01-14 19:30:46,510 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1611003397-127.0.1.1-1421285133099 blk_1073741854_1030 file /tmp/hadoop-doopy/dfs/data/current/BP-1611003397-127.0.1.1-1421285133099/current/finalized/subdir0/subdir0/blk_1073741854
2015-01-14 19:30:49,461 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1611003397-127.0.1.1-1421285133099:blk_1073741855_1031
2015-01-14 19:32:01,490 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.EOFException: End of File Exception between local host is: "barbie/127.0.1.1"; destination host is: "localhost":9000; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:764)
	at org.apache.hadoop.ipc.Client.call(Client.java:1472)
	at org.apache.hadoop.ipc.Client.call(Client.java:1399)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
	at com.sun.proxy.$Proxy11.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:139)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:582)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:680)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:850)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1071)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:966)
2015-01-14 19:32:03,970 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2015-01-14 19:32:03,986 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at barbie/127.0.1.1
************************************************************/
2015-01-14 19:39:24,065 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = barbie/127.0.1.1
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.6.0
STARTUP_MSG:   classpath = /usr/local/hadoop-2.6.0/etc/hadoop:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/commons-collections-3.2.1.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/hadoop-annotations-2.6.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/jasper-runtime-5.5.23.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/curator-client-2.6.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/curator-framework-2.6.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/jsr305-1.3.9.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/curator-recipes-2.6.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/htrace-core-3.0.4.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/commons-el-1.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/jasper-compiler-5.5.23.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/hadoop-auth-2.6.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/slf4j-api-1.7.5.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/hadoop-common-2.6.0-tests.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/hadoop-common-2.6.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/hadoop-nfs-2.6.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/hdfs:/usr/local/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop-2.6.0/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.6.0/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop-2.6.0/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.6.0/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.6.0/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop-2.6.0/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.0/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.0/share/hadoop/hdfs/lib/jasper-runtime-5.5.23.jar:/usr/local/hadoop-2.6.0/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop-2.6.0/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.6.0/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.6.0/share/hadoop/hdfs/lib/jsr305-1.3.9.jar:/usr/local/hadoop-2.6.0/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop-2.6.0/share/hadoop/hdfs/lib/htrace-core-3.0.4.jar:/usr/local/hadoop-2.6.0/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-el-1.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.0/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.6.0/share/hadoop/hdfs/lib/jsp-api-2.1.jar:/usr/local/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-2.6.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-2.6.0-tests.jar:/usr/local/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-nfs-2.6.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/lib/commons-httpclient-3.1.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/lib/jsr305-1.3.9.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/lib/jline-0.9.94.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-registry-2.6.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.6.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-tests-2.6.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.6.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-client-2.6.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.6.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-common-2.6.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.6.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-common-2.6.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.6.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-api-2.6.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.6.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/mapreduce/lib/hadoop-annotations-2.6.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop-2.6.0/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.0/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop-2.6.0/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop-2.6.0/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.0/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.0/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop-2.6.0/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop-2.6.0/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop-2.6.0/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop-2.6.0/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.0/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop-2.6.0/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.6.0/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.0/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.0/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.6.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.6.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.6.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.6.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.6.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.0-tests.jar:/usr/local/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.6.0.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://github.com/apache/hadoop -r e3496499ecb8d220fba99dc5ed4c99c8f9e33bb1; compiled by 'doopy' on 2015-01-14T23:00Z
STARTUP_MSG:   java = 1.7.0_65
************************************************************/
2015-01-14 19:39:24,096 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-01-14 19:39:28,399 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-01-14 19:39:29,256 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-01-14 19:39:29,256 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2015-01-14 19:39:29,296 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is barbie
2015-01-14 19:39:29,387 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2015-01-14 19:39:29,683 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2015-01-14 19:39:29,714 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2015-01-14 19:39:29,715 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2015-01-14 19:39:30,622 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-01-14 19:39:30,669 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2015-01-14 19:39:30,799 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-01-14 19:39:30,830 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2015-01-14 19:39:30,830 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-01-14 19:39:30,830 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-01-14 19:39:30,987 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.datanode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2015-01-14 19:39:31,046 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50075
2015-01-14 19:39:31,046 INFO org.mortbay.log: jetty-6.1.26
2015-01-14 19:39:34,383 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50075
2015-01-14 19:39:34,522 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = doopy
2015-01-14 19:39:34,522 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2015-01-14 19:39:35,093 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-01-14 19:39:35,335 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2015-01-14 19:39:36,176 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2015-01-14 19:39:36,266 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2015-01-14 19:39:36,428 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2015-01-14 19:39:36,532 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:9000 starting to offer service
2015-01-14 19:39:36,650 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-01-14 19:39:36,664 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2015-01-14 19:39:39,831 INFO org.apache.hadoop.hdfs.server.common.Storage: DataNode version: -56 and NameNode layout version: -60
2015-01-14 19:39:39,924 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /tmp/hadoop-doopy/dfs/data/in_use.lock acquired by nodename 3785@barbie
2015-01-14 19:39:40,941 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-1611003397-127.0.1.1-1421285133099
2015-01-14 19:39:40,942 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled
2015-01-14 19:39:40,943 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.
2015-01-14 19:39:40,987 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=131792437;bpid=BP-1611003397-127.0.1.1-1421285133099;lv=-56;nsInfo=lv=-60;cid=CID-a0ed2779-c968-4c23-862a-f2f91301194e;nsid=131792437;c=0;bpid=BP-1611003397-127.0.1.1-1421285133099;dnuuid=47972a9c-c7e0-400a-ad2b-3a4f63bb22be
2015-01-14 19:39:41,447 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: /tmp/hadoop-doopy/dfs/data/current
2015-01-14 19:39:41,478 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /tmp/hadoop-doopy/dfs/data/current, StorageType: DISK
2015-01-14 19:39:42,288 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2015-01-14 19:39:42,336 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1421295922336 with interval 21600000
2015-01-14 19:39:42,340 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-1611003397-127.0.1.1-1421285133099
2015-01-14 19:39:42,360 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-1611003397-127.0.1.1-1421285133099 on volume /tmp/hadoop-doopy/dfs/data/current...
2015-01-14 19:39:42,429 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Cached dfsUsed found for /tmp/hadoop-doopy/dfs/data/current/BP-1611003397-127.0.1.1-1421285133099/current: 102444
2015-01-14 19:39:42,453 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-1611003397-127.0.1.1-1421285133099 on /tmp/hadoop-doopy/dfs/data/current: 93ms
2015-01-14 19:39:42,454 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-1611003397-127.0.1.1-1421285133099: 113ms
2015-01-14 19:39:42,455 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-1611003397-127.0.1.1-1421285133099 on volume /tmp/hadoop-doopy/dfs/data/current...
2015-01-14 19:39:42,560 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-1611003397-127.0.1.1-1421285133099 on volume /tmp/hadoop-doopy/dfs/data/current: 106ms
2015-01-14 19:39:42,560 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 106ms
2015-01-14 19:39:42,581 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-1611003397-127.0.1.1-1421285133099 (Datanode Uuid null) service to localhost/127.0.0.1:9000 beginning handshake with NN
2015-01-14 19:39:42,910 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-1611003397-127.0.1.1-1421285133099 (Datanode Uuid null) service to localhost/127.0.0.1:9000 successfully registered with NN
2015-01-14 19:39:42,910 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode localhost/127.0.0.1:9000 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2015-01-14 19:39:43,892 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Namenode Block pool BP-1611003397-127.0.1.1-1421285133099 (Datanode Uuid 47972a9c-c7e0-400a-ad2b-3a4f63bb22be) service to localhost/127.0.0.1:9000 trying to claim ACTIVE state with txid=210
2015-01-14 19:39:43,893 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode Block pool BP-1611003397-127.0.1.1-1421285133099 (Datanode Uuid 47972a9c-c7e0-400a-ad2b-3a4f63bb22be) service to localhost/127.0.0.1:9000
2015-01-14 19:39:44,481 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Sent 1 blockreports 30 blocks total. Took 51 msec to generate and 536 msecs for RPC and NN processing.  Got back commands org.apache.hadoop.hdfs.server.protocol.FinalizeCommand@4d5427
2015-01-14 19:39:44,481 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1611003397-127.0.1.1-1421285133099
2015-01-14 19:39:44,543 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlockMap
2015-01-14 19:39:44,543 INFO org.apache.hadoop.util.GSet: VM type       = 32-bit
2015-01-14 19:39:44,562 INFO org.apache.hadoop.util.GSet: 0.5% max memory 966.7 MB = 4.8 MB
2015-01-14 19:39:44,562 INFO org.apache.hadoop.util.GSet: capacity      = 2^20 = 1048576 entries
2015-01-14 19:39:44,583 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Periodic Block Verification Scanner initialized with interval 504 hours for block pool BP-1611003397-127.0.1.1-1421285133099
2015-01-14 19:39:44,693 INFO org.apache.hadoop.hdfs.server.datanode.DataBlockScanner: Added bpid=BP-1611003397-127.0.1.1-1421285133099 to blockPoolScannerMap, new size=1
2015-01-14 19:39:47,431 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1611003397-127.0.1.1-1421285133099:blk_1073741853_1029
2015-01-14 19:39:47,432 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1611003397-127.0.1.1-1421285133099:blk_1073741841_1017
2015-01-14 19:39:47,434 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1611003397-127.0.1.1-1421285133099:blk_1073741852_1028
2015-01-14 19:39:47,435 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1611003397-127.0.1.1-1421285133099:blk_1073741851_1027
2015-01-14 19:39:47,436 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1611003397-127.0.1.1-1421285133099:blk_1073741827_1003
2015-01-14 19:39:47,437 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1611003397-127.0.1.1-1421285133099:blk_1073741838_1014
2015-01-14 19:42:12,918 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.EOFException: End of File Exception between local host is: "barbie/127.0.1.1"; destination host is: "localhost":9000; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:764)
	at org.apache.hadoop.ipc.Client.call(Client.java:1472)
	at org.apache.hadoop.ipc.Client.call(Client.java:1399)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
	at com.sun.proxy.$Proxy11.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:139)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:582)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:680)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:850)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1071)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:966)
2015-01-14 19:42:16,758 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2015-01-14 19:42:16,762 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at barbie/127.0.1.1
************************************************************/
2015-01-14 19:46:10,889 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = barbie/127.0.1.1
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.6.0
STARTUP_MSG:   classpath = /usr/local/hadoop-2.6.0/etc/hadoop:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/commons-collections-3.2.1.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/hadoop-annotations-2.6.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/jasper-runtime-5.5.23.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/curator-client-2.6.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/curator-framework-2.6.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/jsr305-1.3.9.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/curator-recipes-2.6.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/htrace-core-3.0.4.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/commons-el-1.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/jasper-compiler-5.5.23.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/hadoop-auth-2.6.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/slf4j-api-1.7.5.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/hadoop-common-2.6.0-tests.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/hadoop-common-2.6.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/hadoop-nfs-2.6.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/hdfs:/usr/local/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop-2.6.0/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.6.0/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop-2.6.0/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.6.0/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.6.0/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop-2.6.0/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.0/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.0/share/hadoop/hdfs/lib/jasper-runtime-5.5.23.jar:/usr/local/hadoop-2.6.0/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop-2.6.0/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.6.0/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.6.0/share/hadoop/hdfs/lib/jsr305-1.3.9.jar:/usr/local/hadoop-2.6.0/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop-2.6.0/share/hadoop/hdfs/lib/htrace-core-3.0.4.jar:/usr/local/hadoop-2.6.0/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-el-1.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.0/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.6.0/share/hadoop/hdfs/lib/jsp-api-2.1.jar:/usr/local/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-2.6.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-2.6.0-tests.jar:/usr/local/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-nfs-2.6.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/lib/commons-httpclient-3.1.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/lib/jsr305-1.3.9.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/lib/jline-0.9.94.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-registry-2.6.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.6.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-tests-2.6.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.6.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-client-2.6.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.6.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-common-2.6.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.6.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-common-2.6.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.6.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-api-2.6.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.6.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/mapreduce/lib/hadoop-annotations-2.6.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop-2.6.0/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.0/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop-2.6.0/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop-2.6.0/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.0/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.0/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop-2.6.0/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop-2.6.0/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop-2.6.0/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop-2.6.0/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.0/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop-2.6.0/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.6.0/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.0/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.0/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.6.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.6.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.6.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.6.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.6.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.0-tests.jar:/usr/local/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.6.0.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://github.com/apache/hadoop -r e3496499ecb8d220fba99dc5ed4c99c8f9e33bb1; compiled by 'doopy' on 2015-01-14T23:00Z
STARTUP_MSG:   java = 1.7.0_65
************************************************************/
2015-01-14 19:46:10,920 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-01-14 19:46:14,930 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-01-14 19:46:15,792 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-01-14 19:46:15,792 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2015-01-14 19:46:15,843 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is barbie
2015-01-14 19:46:15,927 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2015-01-14 19:46:16,223 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2015-01-14 19:46:16,251 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2015-01-14 19:46:16,251 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2015-01-14 19:46:17,070 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-01-14 19:46:17,092 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2015-01-14 19:46:17,163 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-01-14 19:46:17,178 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2015-01-14 19:46:17,184 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-01-14 19:46:17,184 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-01-14 19:46:17,274 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.datanode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2015-01-14 19:46:17,287 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50075
2015-01-14 19:46:17,288 INFO org.mortbay.log: jetty-6.1.26
2015-01-14 19:46:20,135 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50075
2015-01-14 19:46:20,281 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = doopy
2015-01-14 19:46:20,281 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2015-01-14 19:46:20,729 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-01-14 19:46:20,898 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2015-01-14 19:46:21,359 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2015-01-14 19:46:21,411 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2015-01-14 19:46:21,554 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2015-01-14 19:46:21,619 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:9000 starting to offer service
2015-01-14 19:46:21,686 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2015-01-14 19:46:21,697 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-01-14 19:46:24,811 INFO org.apache.hadoop.hdfs.server.common.Storage: DataNode version: -56 and NameNode layout version: -60
2015-01-14 19:46:24,918 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /tmp/hadoop-doopy/dfs/data/in_use.lock acquired by nodename 4624@barbie
2015-01-14 19:46:26,034 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-1611003397-127.0.1.1-1421285133099
2015-01-14 19:46:26,034 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled
2015-01-14 19:46:26,035 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.
2015-01-14 19:46:26,095 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=131792437;bpid=BP-1611003397-127.0.1.1-1421285133099;lv=-56;nsInfo=lv=-60;cid=CID-a0ed2779-c968-4c23-862a-f2f91301194e;nsid=131792437;c=0;bpid=BP-1611003397-127.0.1.1-1421285133099;dnuuid=47972a9c-c7e0-400a-ad2b-3a4f63bb22be
2015-01-14 19:46:26,805 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: /tmp/hadoop-doopy/dfs/data/current
2015-01-14 19:46:26,858 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /tmp/hadoop-doopy/dfs/data/current, StorageType: DISK
2015-01-14 19:46:28,056 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2015-01-14 19:46:28,098 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1421295902098 with interval 21600000
2015-01-14 19:46:28,120 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-1611003397-127.0.1.1-1421285133099
2015-01-14 19:46:28,128 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-1611003397-127.0.1.1-1421285133099 on volume /tmp/hadoop-doopy/dfs/data/current...
2015-01-14 19:46:28,205 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Cached dfsUsed found for /tmp/hadoop-doopy/dfs/data/current/BP-1611003397-127.0.1.1-1421285133099/current: 102444
2015-01-14 19:46:28,213 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-1611003397-127.0.1.1-1421285133099 on /tmp/hadoop-doopy/dfs/data/current: 85ms
2015-01-14 19:46:28,232 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-1611003397-127.0.1.1-1421285133099: 112ms
2015-01-14 19:46:28,233 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-1611003397-127.0.1.1-1421285133099 on volume /tmp/hadoop-doopy/dfs/data/current...
2015-01-14 19:46:28,312 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-1611003397-127.0.1.1-1421285133099 on volume /tmp/hadoop-doopy/dfs/data/current: 78ms
2015-01-14 19:46:28,331 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 99ms
2015-01-14 19:46:28,345 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-1611003397-127.0.1.1-1421285133099 (Datanode Uuid null) service to localhost/127.0.0.1:9000 beginning handshake with NN
2015-01-14 19:46:28,655 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-1611003397-127.0.1.1-1421285133099 (Datanode Uuid null) service to localhost/127.0.0.1:9000 successfully registered with NN
2015-01-14 19:46:28,656 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode localhost/127.0.0.1:9000 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2015-01-14 19:46:29,688 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Namenode Block pool BP-1611003397-127.0.1.1-1421285133099 (Datanode Uuid 47972a9c-c7e0-400a-ad2b-3a4f63bb22be) service to localhost/127.0.0.1:9000 trying to claim ACTIVE state with txid=213
2015-01-14 19:46:29,688 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode Block pool BP-1611003397-127.0.1.1-1421285133099 (Datanode Uuid 47972a9c-c7e0-400a-ad2b-3a4f63bb22be) service to localhost/127.0.0.1:9000
2015-01-14 19:46:30,273 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Sent 1 blockreports 30 blocks total. Took 20 msec to generate and 560 msecs for RPC and NN processing.  Got back commands org.apache.hadoop.hdfs.server.protocol.FinalizeCommand@101d92f
2015-01-14 19:46:30,274 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1611003397-127.0.1.1-1421285133099
2015-01-14 19:46:30,329 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlockMap
2015-01-14 19:46:30,329 INFO org.apache.hadoop.util.GSet: VM type       = 32-bit
2015-01-14 19:46:30,360 INFO org.apache.hadoop.util.GSet: 0.5% max memory 966.7 MB = 4.8 MB
2015-01-14 19:46:30,360 INFO org.apache.hadoop.util.GSet: capacity      = 2^20 = 1048576 entries
2015-01-14 19:46:30,374 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Periodic Block Verification Scanner initialized with interval 504 hours for block pool BP-1611003397-127.0.1.1-1421285133099
2015-01-14 19:46:30,497 INFO org.apache.hadoop.hdfs.server.datanode.DataBlockScanner: Added bpid=BP-1611003397-127.0.1.1-1421285133099 to blockPoolScannerMap, new size=1
2015-01-14 19:46:33,240 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1611003397-127.0.1.1-1421285133099:blk_1073741847_1023
2015-01-14 19:46:33,241 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1611003397-127.0.1.1-1421285133099:blk_1073741834_1010
2015-01-14 19:46:33,243 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1611003397-127.0.1.1-1421285133099:blk_1073741825_1001
2015-01-14 19:46:33,245 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1611003397-127.0.1.1-1421285133099:blk_1073741833_1009
2015-01-14 19:46:33,247 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1611003397-127.0.1.1-1421285133099:blk_1073741846_1022
2015-01-14 19:47:59,211 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1611003397-127.0.1.1-1421285133099:blk_1073741856_1032 src: /127.0.0.1:59484 dest: /127.0.0.1:50010
2015-01-14 19:47:59,461 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:59484, dest: /127.0.0.1:50010, bytes: 3512, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_86210742_1, offset: 0, srvID: 47972a9c-c7e0-400a-ad2b-3a4f63bb22be, blockid: BP-1611003397-127.0.1.1-1421285133099:blk_1073741856_1032, duration: 186135666
2015-01-14 19:47:59,472 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1611003397-127.0.1.1-1421285133099:blk_1073741856_1032, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-01-14 19:47:59,946 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1611003397-127.0.1.1-1421285133099:blk_1073741857_1033 src: /127.0.0.1:59485 dest: /127.0.0.1:50010
2015-01-14 19:47:59,957 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:59485, dest: /127.0.0.1:50010, bytes: 407, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_86210742_1, offset: 0, srvID: 47972a9c-c7e0-400a-ad2b-3a4f63bb22be, blockid: BP-1611003397-127.0.1.1-1421285133099:blk_1073741857_1033, duration: 3602583
2015-01-14 19:47:59,961 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1611003397-127.0.1.1-1421285133099:blk_1073741857_1033, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-01-14 19:48:00,246 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1611003397-127.0.1.1-1421285133099:blk_1073741858_1034 src: /127.0.0.1:59486 dest: /127.0.0.1:50010
2015-01-14 19:48:00,259 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:59486, dest: /127.0.0.1:50010, bytes: 87578, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_86210742_1, offset: 0, srvID: 47972a9c-c7e0-400a-ad2b-3a4f63bb22be, blockid: BP-1611003397-127.0.1.1-1421285133099:blk_1073741858_1034, duration: 5963839
2015-01-14 19:48:00,263 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1611003397-127.0.1.1-1421285133099:blk_1073741858_1034, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-01-14 19:48:09,203 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1611003397-127.0.1.1-1421285133099:blk_1073741858_1034
2015-01-14 19:48:09,204 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1611003397-127.0.1.1-1421285133099:blk_1073741857_1033
2015-01-14 19:48:09,206 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1611003397-127.0.1.1-1421285133099:blk_1073741856_1032
2015-01-14 19:48:17,221 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1611003397-127.0.1.1-1421285133099:blk_1073741859_1035 src: /127.0.0.1:59495 dest: /127.0.0.1:50010
2015-01-14 19:48:17,292 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:59495, dest: /127.0.0.1:50010, bytes: 104172, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_748411284_1, offset: 0, srvID: 47972a9c-c7e0-400a-ad2b-3a4f63bb22be, blockid: BP-1611003397-127.0.1.1-1421285133099:blk_1073741859_1035, duration: 63838132
2015-01-14 19:48:17,292 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1611003397-127.0.1.1-1421285133099:blk_1073741859_1035, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-01-14 19:48:24,468 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1611003397-127.0.1.1-1421285133099:blk_1073741859_1035
2015-01-14 19:49:51,831 WARN org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 14745ms
No GCs detected
2015-01-14 19:50:51,325 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1611003397-127.0.1.1-1421285133099:blk_1073741860_1036 src: /127.0.0.1:59573 dest: /127.0.0.1:50010
2015-01-14 19:50:56,947 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 4919ms
GC pool 'Copy' had collection(s): count=1 time=4861ms
2015-01-14 19:52:00,925 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 2388ms
No GCs detected
2015-01-14 19:52:06,525 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1078ms
No GCs detected
2015-01-14 19:52:18,043 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1332ms
No GCs detected
2015-01-14 19:54:08,780 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 4606ms
No GCs detected
2015-01-14 19:54:15,300 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1836ms
No GCs detected
2015-01-14 19:55:33,772 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 2574ms
GC pool 'Copy' had collection(s): count=1 time=2727ms
2015-01-14 19:56:11,213 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1825ms
No GCs detected
2015-01-14 19:56:20,084 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1585ms
No GCs detected
2015-01-14 19:56:21,791 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1207ms
No GCs detected
2015-01-14 19:56:23,867 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1574ms
No GCs detected
2015-01-14 19:56:26,423 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1059ms
No GCs detected
2015-01-14 19:56:32,538 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 2419ms
No GCs detected
2015-01-14 19:57:59,620 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 2950ms
GC pool 'Copy' had collection(s): count=1 time=3385ms
2015-01-14 19:58:32,869 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 8009ms
No GCs detected
2015-01-14 19:58:39,495 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1472ms
No GCs detected
2015-01-14 19:59:43,442 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow BlockReceiver write data to disk cost:811ms (threshold=300ms)
2015-01-14 20:00:20,318 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1062ms
GC pool 'Copy' had collection(s): count=1 time=1349ms
2015-01-14 20:00:55,429 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 8042ms
No GCs detected
2015-01-14 20:00:59,579 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1376ms
No GCs detected
2015-01-14 20:01:03,863 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1736ms
No GCs detected
2015-01-14 20:01:54,678 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 5973ms
GC pool 'Copy' had collection(s): count=1 time=5763ms
2015-01-14 20:02:01,052 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1372ms
No GCs detected
2015-01-14 20:02:04,802 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Slow PacketResponder send ack to upstream took 311ms (threshold=300ms), PacketResponder: BP-1611003397-127.0.1.1-1421285133099:blk_1073741860_1036, type=LAST_IN_PIPELINE, downstreams=0:[], replyAck=seqno: -1 status: SUCCESS downstreamAckTimeNanos: 0
2015-01-14 20:02:12,100 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Exception for BP-1611003397-127.0.1.1-1421285133099:blk_1073741860_1036
java.io.IOException: Connection reset by peer
	at sun.nio.ch.FileDispatcherImpl.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)
	at sun.nio.ch.IOUtil.read(IOUtil.java:197)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:379)
	at org.apache.hadoop.net.SocketInputStream$Reader.performIO(SocketInputStream.java:57)
	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:142)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
	at java.io.BufferedInputStream.fill(BufferedInputStream.java:235)
	at java.io.BufferedInputStream.read1(BufferedInputStream.java:275)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:334)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.io.IOUtils.readFully(IOUtils.java:192)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doReadFully(PacketReceiver.java:213)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doRead(PacketReceiver.java:134)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.receiveNextPacket(PacketReceiver.java:109)
	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receivePacket(BlockReceiver.java:467)
	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receiveBlock(BlockReceiver.java:781)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.writeBlock(DataXceiver.java:730)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opWriteBlock(Receiver.java:137)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:74)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:235)
	at java.lang.Thread.run(Thread.java:745)
2015-01-14 20:02:13,813 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1611003397-127.0.1.1-1421285133099:blk_1073741860_1036, type=LAST_IN_PIPELINE, downstreams=0:[]: Thread is interrupted.
2015-01-14 20:02:13,813 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1611003397-127.0.1.1-1421285133099:blk_1073741860_1036, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-01-14 20:02:13,813 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: opWriteBlock BP-1611003397-127.0.1.1-1421285133099:blk_1073741860_1036 received exception java.io.IOException: Connection reset by peer
2015-01-14 20:02:13,813 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: barbie:50010:DataXceiver error processing WRITE_BLOCK operation  src: /127.0.0.1:59573 dst: /127.0.0.1:50010
java.io.IOException: Connection reset by peer
	at sun.nio.ch.FileDispatcherImpl.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)
	at sun.nio.ch.IOUtil.read(IOUtil.java:197)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:379)
	at org.apache.hadoop.net.SocketInputStream$Reader.performIO(SocketInputStream.java:57)
	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:142)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
	at java.io.BufferedInputStream.fill(BufferedInputStream.java:235)
	at java.io.BufferedInputStream.read1(BufferedInputStream.java:275)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:334)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.io.IOUtils.readFully(IOUtils.java:192)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doReadFully(PacketReceiver.java:213)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doRead(PacketReceiver.java:134)
	at org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.receiveNextPacket(PacketReceiver.java:109)
	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receivePacket(BlockReceiver.java:467)
	at org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receiveBlock(BlockReceiver.java:781)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.writeBlock(DataXceiver.java:730)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opWriteBlock(Receiver.java:137)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:74)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:235)
	at java.lang.Thread.run(Thread.java:745)
2015-01-14 20:02:48,955 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1611003397-127.0.1.1-1421285133099:blk_1073741861_1037 src: /127.0.0.1:59909 dest: /127.0.0.1:50010
2015-01-14 20:02:49,119 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:59909, dest: /127.0.0.1:50010, bytes: 457, op: HDFS_WRITE, cliID: DFSClient_attempt_1421286406893_0001_r_000000_0_-836744152_1, offset: 0, srvID: 47972a9c-c7e0-400a-ad2b-3a4f63bb22be, blockid: BP-1611003397-127.0.1.1-1421285133099:blk_1073741861_1037, duration: 117389306
2015-01-14 20:02:49,120 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1611003397-127.0.1.1-1421285133099:blk_1073741861_1037, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-01-14 20:02:58,113 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1611003397-127.0.1.1-1421285133099:blk_1073741861_1037
2015-01-14 20:03:00,441 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741856_1032 file /tmp/hadoop-doopy/dfs/data/current/BP-1611003397-127.0.1.1-1421285133099/current/finalized/subdir0/subdir0/blk_1073741856 for deletion
2015-01-14 20:03:00,529 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741857_1033 file /tmp/hadoop-doopy/dfs/data/current/BP-1611003397-127.0.1.1-1421285133099/current/finalized/subdir0/subdir0/blk_1073741857 for deletion
2015-01-14 20:03:00,583 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741858_1034 file /tmp/hadoop-doopy/dfs/data/current/BP-1611003397-127.0.1.1-1421285133099/current/finalized/subdir0/subdir0/blk_1073741858 for deletion
2015-01-14 20:03:00,583 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741859_1035 file /tmp/hadoop-doopy/dfs/data/current/BP-1611003397-127.0.1.1-1421285133099/current/finalized/subdir0/subdir0/blk_1073741859 for deletion
2015-01-14 20:03:00,530 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1611003397-127.0.1.1-1421285133099 blk_1073741856_1032 file /tmp/hadoop-doopy/dfs/data/current/BP-1611003397-127.0.1.1-1421285133099/current/finalized/subdir0/subdir0/blk_1073741856
2015-01-14 20:03:00,584 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1611003397-127.0.1.1-1421285133099 blk_1073741857_1033 file /tmp/hadoop-doopy/dfs/data/current/BP-1611003397-127.0.1.1-1421285133099/current/finalized/subdir0/subdir0/blk_1073741857
2015-01-14 20:03:00,584 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1611003397-127.0.1.1-1421285133099 blk_1073741858_1034 file /tmp/hadoop-doopy/dfs/data/current/BP-1611003397-127.0.1.1-1421285133099/current/finalized/subdir0/subdir0/blk_1073741858
2015-01-14 20:03:00,585 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1611003397-127.0.1.1-1421285133099 blk_1073741859_1035 file /tmp/hadoop-doopy/dfs/data/current/BP-1611003397-127.0.1.1-1421285133099/current/finalized/subdir0/subdir0/blk_1073741859
2015-01-14 20:03:06,041 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1611003397-127.0.1.1-1421285133099:blk_1073741862_1038 src: /127.0.0.1:59913 dest: /127.0.0.1:50010
2015-01-14 20:03:06,073 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:59913, dest: /127.0.0.1:50010, bytes: 137, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_86210742_1, offset: 0, srvID: 47972a9c-c7e0-400a-ad2b-3a4f63bb22be, blockid: BP-1611003397-127.0.1.1-1421285133099:blk_1073741862_1038, duration: 27039036
2015-01-14 20:03:06,074 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1611003397-127.0.1.1-1421285133099:blk_1073741862_1038, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-01-14 20:03:06,129 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1611003397-127.0.1.1-1421285133099:blk_1073741863_1039 src: /127.0.0.1:59914 dest: /127.0.0.1:50010
2015-01-14 20:03:06,138 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:59914, dest: /127.0.0.1:50010, bytes: 22, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_86210742_1, offset: 0, srvID: 47972a9c-c7e0-400a-ad2b-3a4f63bb22be, blockid: BP-1611003397-127.0.1.1-1421285133099:blk_1073741863_1039, duration: 3701110
2015-01-14 20:03:06,141 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1611003397-127.0.1.1-1421285133099:blk_1073741863_1039, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-01-14 20:03:06,371 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1611003397-127.0.1.1-1421285133099:blk_1073741864_1040 src: /127.0.0.1:59915 dest: /127.0.0.1:50010
2015-01-14 20:03:06,449 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:59915, dest: /127.0.0.1:50010, bytes: 87142, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_86210742_1, offset: 0, srvID: 47972a9c-c7e0-400a-ad2b-3a4f63bb22be, blockid: BP-1611003397-127.0.1.1-1421285133099:blk_1073741864_1040, duration: 74706422
2015-01-14 20:03:06,451 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1611003397-127.0.1.1-1421285133099:blk_1073741864_1040, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-01-14 20:03:13,130 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1611003397-127.0.1.1-1421285133099:blk_1073741864_1040
2015-01-14 20:03:13,131 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1611003397-127.0.1.1-1421285133099:blk_1073741863_1039
2015-01-14 20:03:13,132 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1611003397-127.0.1.1-1421285133099:blk_1073741862_1038
2015-01-14 20:05:55,131 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1611003397-127.0.1.1-1421285133099:blk_1073741865_1041 src: /127.0.0.1:59929 dest: /127.0.0.1:50010
2015-01-14 20:05:55,228 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:59929, dest: /127.0.0.1:50010, bytes: 3512, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1385649367_1, offset: 0, srvID: 47972a9c-c7e0-400a-ad2b-3a4f63bb22be, blockid: BP-1611003397-127.0.1.1-1421285133099:blk_1073741865_1041, duration: 92121169
2015-01-14 20:05:55,229 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1611003397-127.0.1.1-1421285133099:blk_1073741865_1041, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-01-14 20:05:55,347 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1611003397-127.0.1.1-1421285133099:blk_1073741866_1042 src: /127.0.0.1:59930 dest: /127.0.0.1:50010
2015-01-14 20:05:55,356 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:59930, dest: /127.0.0.1:50010, bytes: 407, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1385649367_1, offset: 0, srvID: 47972a9c-c7e0-400a-ad2b-3a4f63bb22be, blockid: BP-1611003397-127.0.1.1-1421285133099:blk_1073741866_1042, duration: 3445388
2015-01-14 20:05:55,359 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1611003397-127.0.1.1-1421285133099:blk_1073741866_1042, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-01-14 20:05:55,880 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1611003397-127.0.1.1-1421285133099:blk_1073741867_1043 src: /127.0.0.1:59931 dest: /127.0.0.1:50010
2015-01-14 20:05:55,893 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:59931, dest: /127.0.0.1:50010, bytes: 87579, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1385649367_1, offset: 0, srvID: 47972a9c-c7e0-400a-ad2b-3a4f63bb22be, blockid: BP-1611003397-127.0.1.1-1421285133099:blk_1073741867_1043, duration: 6082595
2015-01-14 20:05:55,896 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1611003397-127.0.1.1-1421285133099:blk_1073741867_1043, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-01-14 20:06:03,232 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1611003397-127.0.1.1-1421285133099:blk_1073741867_1043
2015-01-14 20:06:03,233 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1611003397-127.0.1.1-1421285133099:blk_1073741866_1042
2015-01-14 20:06:03,234 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1611003397-127.0.1.1-1421285133099:blk_1073741865_1041
2015-01-14 20:06:17,991 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1611003397-127.0.1.1-1421285133099:blk_1073741868_1044 src: /127.0.0.1:59935 dest: /127.0.0.1:50010
2015-01-14 20:06:18,079 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:59935, dest: /127.0.0.1:50010, bytes: 3512, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_2145073534_1, offset: 0, srvID: 47972a9c-c7e0-400a-ad2b-3a4f63bb22be, blockid: BP-1611003397-127.0.1.1-1421285133099:blk_1073741868_1044, duration: 82469110
2015-01-14 20:06:18,079 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1611003397-127.0.1.1-1421285133099:blk_1073741868_1044, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-01-14 20:06:18,144 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1611003397-127.0.1.1-1421285133099:blk_1073741869_1045 src: /127.0.0.1:59936 dest: /127.0.0.1:50010
2015-01-14 20:06:18,154 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:59936, dest: /127.0.0.1:50010, bytes: 407, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_2145073534_1, offset: 0, srvID: 47972a9c-c7e0-400a-ad2b-3a4f63bb22be, blockid: BP-1611003397-127.0.1.1-1421285133099:blk_1073741869_1045, duration: 3374762
2015-01-14 20:06:18,157 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1611003397-127.0.1.1-1421285133099:blk_1073741869_1045, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-01-14 20:06:18,511 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1611003397-127.0.1.1-1421285133099:blk_1073741870_1046 src: /127.0.0.1:59937 dest: /127.0.0.1:50010
2015-01-14 20:06:18,553 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:59937, dest: /127.0.0.1:50010, bytes: 87579, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_2145073534_1, offset: 0, srvID: 47972a9c-c7e0-400a-ad2b-3a4f63bb22be, blockid: BP-1611003397-127.0.1.1-1421285133099:blk_1073741870_1046, duration: 37749003
2015-01-14 20:06:18,557 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1611003397-127.0.1.1-1421285133099:blk_1073741870_1046, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-01-14 20:06:23,249 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1611003397-127.0.1.1-1421285133099:blk_1073741868_1044
2015-01-14 20:06:23,250 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1611003397-127.0.1.1-1421285133099:blk_1073741869_1045
2015-01-14 20:06:28,261 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1611003397-127.0.1.1-1421285133099:blk_1073741870_1046
2015-01-14 20:13:41,734 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1611003397-127.0.1.1-1421285133099:blk_1073741871_1047 src: /127.0.0.1:59976 dest: /127.0.0.1:50010
2015-01-14 20:13:41,803 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:59976, dest: /127.0.0.1:50010, bytes: 103664, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_541068657_1, offset: 0, srvID: 47972a9c-c7e0-400a-ad2b-3a4f63bb22be, blockid: BP-1611003397-127.0.1.1-1421285133099:blk_1073741871_1047, duration: 64048880
2015-01-14 20:13:41,803 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1611003397-127.0.1.1-1421285133099:blk_1073741871_1047, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-01-14 20:13:48,567 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1611003397-127.0.1.1-1421285133099:blk_1073741871_1047
2015-01-14 20:13:49,363 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1611003397-127.0.1.1-1421285133099:blk_1073741872_1048 src: /127.0.0.1:59982 dest: /127.0.0.1:50010
2015-01-14 20:13:57,696 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1611003397-127.0.1.1-1421285133099:blk_1073741873_1049 src: /127.0.0.1:59987 dest: /127.0.0.1:50010
2015-01-14 20:13:57,783 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:59987, dest: /127.0.0.1:50010, bytes: 203, op: HDFS_WRITE, cliID: DFSClient_attempt_1421286406893_0002_r_000000_0_-879256423_1, offset: 0, srvID: 47972a9c-c7e0-400a-ad2b-3a4f63bb22be, blockid: BP-1611003397-127.0.1.1-1421285133099:blk_1073741873_1049, duration: 81694773
2015-01-14 20:13:57,783 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1611003397-127.0.1.1-1421285133099:blk_1073741873_1049, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-01-14 20:13:58,293 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:59982, dest: /127.0.0.1:50010, bytes: 33285, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_541068657_1, offset: 0, srvID: 47972a9c-c7e0-400a-ad2b-3a4f63bb22be, blockid: BP-1611003397-127.0.1.1-1421285133099:blk_1073741872_1048, duration: 8915046354
2015-01-14 20:13:58,296 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1611003397-127.0.1.1-1421285133099:blk_1073741872_1048, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-01-14 20:13:58,378 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1611003397-127.0.1.1-1421285133099:blk_1073741874_1050 src: /127.0.0.1:59989 dest: /127.0.0.1:50010
2015-01-14 20:13:58,386 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:59989, dest: /127.0.0.1:50010, bytes: 347, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_541068657_1, offset: 0, srvID: 47972a9c-c7e0-400a-ad2b-3a4f63bb22be, blockid: BP-1611003397-127.0.1.1-1421285133099:blk_1073741874_1050, duration: 3159533
2015-01-14 20:13:58,390 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1611003397-127.0.1.1-1421285133099:blk_1073741874_1050, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-01-14 20:13:58,503 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1611003397-127.0.1.1-1421285133099:blk_1073741875_1051 src: /127.0.0.1:59991 dest: /127.0.0.1:50010
2015-01-14 20:13:58,513 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:59991, dest: /127.0.0.1:50010, bytes: 33285, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_541068657_1, offset: 0, srvID: 47972a9c-c7e0-400a-ad2b-3a4f63bb22be, blockid: BP-1611003397-127.0.1.1-1421285133099:blk_1073741875_1051, duration: 4168431
2015-01-14 20:13:58,516 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1611003397-127.0.1.1-1421285133099:blk_1073741875_1051, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-01-14 20:13:58,604 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1611003397-127.0.1.1-1421285133099:blk_1073741876_1052 src: /127.0.0.1:59992 dest: /127.0.0.1:50010
2015-01-14 20:13:58,614 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:59992, dest: /127.0.0.1:50010, bytes: 103664, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_541068657_1, offset: 0, srvID: 47972a9c-c7e0-400a-ad2b-3a4f63bb22be, blockid: BP-1611003397-127.0.1.1-1421285133099:blk_1073741876_1052, duration: 5616822
2015-01-14 20:13:58,617 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1611003397-127.0.1.1-1421285133099:blk_1073741876_1052, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-01-14 20:14:02,518 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741872_1048 file /tmp/hadoop-doopy/dfs/data/current/BP-1611003397-127.0.1.1-1421285133099/current/finalized/subdir0/subdir0/blk_1073741872 for deletion
2015-01-14 20:14:02,518 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741862_1038 file /tmp/hadoop-doopy/dfs/data/current/BP-1611003397-127.0.1.1-1421285133099/current/finalized/subdir0/subdir0/blk_1073741862 for deletion
2015-01-14 20:14:02,518 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741863_1039 file /tmp/hadoop-doopy/dfs/data/current/BP-1611003397-127.0.1.1-1421285133099/current/finalized/subdir0/subdir0/blk_1073741863 for deletion
2015-01-14 20:14:02,519 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741864_1040 file /tmp/hadoop-doopy/dfs/data/current/BP-1611003397-127.0.1.1-1421285133099/current/finalized/subdir0/subdir0/blk_1073741864 for deletion
2015-01-14 20:14:02,519 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741871_1047 file /tmp/hadoop-doopy/dfs/data/current/BP-1611003397-127.0.1.1-1421285133099/current/finalized/subdir0/subdir0/blk_1073741871 for deletion
2015-01-14 20:14:02,519 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1611003397-127.0.1.1-1421285133099 blk_1073741872_1048 file /tmp/hadoop-doopy/dfs/data/current/BP-1611003397-127.0.1.1-1421285133099/current/finalized/subdir0/subdir0/blk_1073741872
2015-01-14 20:14:02,520 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1611003397-127.0.1.1-1421285133099 blk_1073741862_1038 file /tmp/hadoop-doopy/dfs/data/current/BP-1611003397-127.0.1.1-1421285133099/current/finalized/subdir0/subdir0/blk_1073741862
2015-01-14 20:14:02,520 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1611003397-127.0.1.1-1421285133099 blk_1073741863_1039 file /tmp/hadoop-doopy/dfs/data/current/BP-1611003397-127.0.1.1-1421285133099/current/finalized/subdir0/subdir0/blk_1073741863
2015-01-14 20:14:02,520 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1611003397-127.0.1.1-1421285133099 blk_1073741864_1040 file /tmp/hadoop-doopy/dfs/data/current/BP-1611003397-127.0.1.1-1421285133099/current/finalized/subdir0/subdir0/blk_1073741864
2015-01-14 20:14:02,521 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1611003397-127.0.1.1-1421285133099 blk_1073741871_1047 file /tmp/hadoop-doopy/dfs/data/current/BP-1611003397-127.0.1.1-1421285133099/current/finalized/subdir0/subdir0/blk_1073741871
2015-01-14 20:14:03,605 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1611003397-127.0.1.1-1421285133099:blk_1073741875_1051
2015-01-14 20:14:03,606 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1611003397-127.0.1.1-1421285133099:blk_1073741874_1050
2015-01-14 20:14:03,607 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1611003397-127.0.1.1-1421285133099:blk_1073741873_1049
2015-01-14 20:14:08,617 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1611003397-127.0.1.1-1421285133099:blk_1073741876_1052
2015-01-14 20:14:13,831 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1611003397-127.0.1.1-1421285133099:blk_1073741877_1053 src: /127.0.0.1:60001 dest: /127.0.0.1:50010
2015-01-14 20:14:13,920 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:60001, dest: /127.0.0.1:50010, bytes: 104173, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-2135711393_1, offset: 0, srvID: 47972a9c-c7e0-400a-ad2b-3a4f63bb22be, blockid: BP-1611003397-127.0.1.1-1421285133099:blk_1073741877_1053, duration: 83380978
2015-01-14 20:14:13,920 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1611003397-127.0.1.1-1421285133099:blk_1073741877_1053, type=LAST_IN_PIPELINE, downstreams=0:[] terminating
2015-01-14 20:14:23,652 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Verification succeeded for BP-1611003397-127.0.1.1-1421285133099:blk_1073741877_1053
2015-01-14 20:15:12,647 WARN org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 10585ms
No GCs detected
2015-01-14 20:15:20,296 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1039ms
No GCs detected
2015-01-14 20:15:22,140 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1343ms
No GCs detected
2015-01-14 20:15:32,324 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1452ms
No GCs detected
2015-01-14 20:15:37,801 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1002ms
No GCs detected
2015-01-14 20:18:51,983 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.EOFException: End of File Exception between local host is: "barbie/127.0.1.1"; destination host is: "localhost":9000; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:764)
	at org.apache.hadoop.ipc.Client.call(Client.java:1472)
	at org.apache.hadoop.ipc.Client.call(Client.java:1399)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
	at com.sun.proxy.$Proxy11.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:139)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:582)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:680)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:850)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1071)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:966)
2015-01-14 20:18:54,674 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2015-01-14 20:18:54,780 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at barbie/127.0.1.1
************************************************************/
