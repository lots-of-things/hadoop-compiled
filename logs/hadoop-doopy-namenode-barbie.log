2015-01-14 19:28:05,042 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting NameNode
STARTUP_MSG:   host = barbie/127.0.1.1
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.6.0
STARTUP_MSG:   classpath = /usr/local/hadoop-2.6.0/etc/hadoop:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/commons-collections-3.2.1.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/hadoop-annotations-2.6.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/jasper-runtime-5.5.23.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/curator-client-2.6.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/curator-framework-2.6.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/jsr305-1.3.9.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/curator-recipes-2.6.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/htrace-core-3.0.4.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/commons-el-1.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/jasper-compiler-5.5.23.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/hadoop-auth-2.6.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/slf4j-api-1.7.5.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/hadoop-common-2.6.0-tests.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/hadoop-common-2.6.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/hadoop-nfs-2.6.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/hdfs:/usr/local/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop-2.6.0/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.6.0/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop-2.6.0/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.6.0/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.6.0/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop-2.6.0/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.0/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.0/share/hadoop/hdfs/lib/jasper-runtime-5.5.23.jar:/usr/local/hadoop-2.6.0/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop-2.6.0/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.6.0/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.6.0/share/hadoop/hdfs/lib/jsr305-1.3.9.jar:/usr/local/hadoop-2.6.0/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop-2.6.0/share/hadoop/hdfs/lib/htrace-core-3.0.4.jar:/usr/local/hadoop-2.6.0/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-el-1.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.0/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.6.0/share/hadoop/hdfs/lib/jsp-api-2.1.jar:/usr/local/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-2.6.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-2.6.0-tests.jar:/usr/local/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-nfs-2.6.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/lib/commons-httpclient-3.1.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/lib/jsr305-1.3.9.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/lib/jline-0.9.94.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-registry-2.6.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.6.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-tests-2.6.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.6.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-client-2.6.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.6.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-common-2.6.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.6.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-common-2.6.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.6.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-api-2.6.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.6.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/mapreduce/lib/hadoop-annotations-2.6.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop-2.6.0/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.0/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop-2.6.0/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop-2.6.0/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.0/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.0/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop-2.6.0/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop-2.6.0/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop-2.6.0/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop-2.6.0/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.0/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop-2.6.0/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.6.0/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.0/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.0/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.6.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.6.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.6.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.6.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.6.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.0-tests.jar:/usr/local/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.6.0.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://github.com/apache/hadoop -r e3496499ecb8d220fba99dc5ed4c99c8f9e33bb1; compiled by 'doopy' on 2015-01-14T23:00Z
STARTUP_MSG:   java = 1.7.0_65
************************************************************/
2015-01-14 19:28:05,065 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-01-14 19:28:05,072 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: createNameNode []
2015-01-14 19:28:05,992 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-01-14 19:28:06,348 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-01-14 19:28:06,349 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: NameNode metrics system started
2015-01-14 19:28:06,352 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: fs.defaultFS is hdfs://localhost:9000
2015-01-14 19:28:06,352 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Clients are to use localhost:9000 to access this namenode/service.
2015-01-14 19:28:06,793 INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for hdfs at: http://0.0.0.0:50070
2015-01-14 19:28:06,905 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-01-14 19:28:06,914 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.namenode is not defined
2015-01-14 19:28:06,936 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-01-14 19:28:06,944 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs
2015-01-14 19:28:06,944 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-01-14 19:28:06,944 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-01-14 19:28:07,006 INFO org.apache.hadoop.http.HttpServer2: Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)
2015-01-14 19:28:07,009 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2015-01-14 19:28:07,074 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50070
2015-01-14 19:28:07,074 INFO org.mortbay.log: jetty-6.1.26
2015-01-14 19:28:07,589 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50070
2015-01-14 19:28:07,680 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one image storage directory (dfs.namenode.name.dir) configured. Beware of data loss due to lack of redundant storage directories!
2015-01-14 19:28:07,680 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one namespace edits storage directory (dfs.namenode.edits.dir) configured. Beware of data loss due to lack of redundant storage directories!
2015-01-14 19:28:07,768 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: No KeyProvider found.
2015-01-14 19:28:07,793 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair:true
2015-01-14 19:28:07,874 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit=1000
2015-01-14 19:28:07,874 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
2015-01-14 19:28:07,881 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2015-01-14 19:28:07,883 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: The block deletion will start around 2015 Jan 14 19:28:07
2015-01-14 19:28:07,887 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlocksMap
2015-01-14 19:28:07,887 INFO org.apache.hadoop.util.GSet: VM type       = 32-bit
2015-01-14 19:28:07,891 INFO org.apache.hadoop.util.GSet: 2.0% max memory 966.7 MB = 19.3 MB
2015-01-14 19:28:07,891 INFO org.apache.hadoop.util.GSet: capacity      = 2^22 = 4194304 entries
2015-01-14 19:28:07,980 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable=false
2015-01-14 19:28:07,980 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: defaultReplication         = 1
2015-01-14 19:28:07,980 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplication             = 512
2015-01-14 19:28:07,980 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: minReplication             = 1
2015-01-14 19:28:07,981 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplicationStreams      = 2
2015-01-14 19:28:07,981 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: shouldCheckForEnoughRacks  = false
2015-01-14 19:28:07,981 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: replicationRecheckInterval = 3000
2015-01-14 19:28:07,981 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: encryptDataTransfer        = false
2015-01-14 19:28:07,981 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
2015-01-14 19:28:07,994 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             = doopy (auth:SIMPLE)
2015-01-14 19:28:07,994 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          = supergroup
2015-01-14 19:28:07,994 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled = true
2015-01-14 19:28:07,994 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled: false
2015-01-14 19:28:07,998 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Append Enabled: true
2015-01-14 19:28:08,100 INFO org.apache.hadoop.util.GSet: Computing capacity for map INodeMap
2015-01-14 19:28:08,100 INFO org.apache.hadoop.util.GSet: VM type       = 32-bit
2015-01-14 19:28:08,100 INFO org.apache.hadoop.util.GSet: 1.0% max memory 966.7 MB = 9.7 MB
2015-01-14 19:28:08,100 INFO org.apache.hadoop.util.GSet: capacity      = 2^21 = 2097152 entries
2015-01-14 19:28:08,188 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Caching file names occuring more than 10 times
2015-01-14 19:28:08,206 INFO org.apache.hadoop.util.GSet: Computing capacity for map cachedBlocks
2015-01-14 19:28:08,206 INFO org.apache.hadoop.util.GSet: VM type       = 32-bit
2015-01-14 19:28:08,207 INFO org.apache.hadoop.util.GSet: 0.25% max memory 966.7 MB = 2.4 MB
2015-01-14 19:28:08,207 INFO org.apache.hadoop.util.GSet: capacity      = 2^19 = 524288 entries
2015-01-14 19:28:08,211 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2015-01-14 19:28:08,211 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.min.datanodes = 0
2015-01-14 19:28:08,211 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.extension     = 30000
2015-01-14 19:28:08,214 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache on namenode is enabled
2015-01-14 19:28:08,214 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2015-01-14 19:28:08,218 INFO org.apache.hadoop.util.GSet: Computing capacity for map NameNodeRetryCache
2015-01-14 19:28:08,219 INFO org.apache.hadoop.util.GSet: VM type       = 32-bit
2015-01-14 19:28:08,219 INFO org.apache.hadoop.util.GSet: 0.029999999329447746% max memory 966.7 MB = 297.0 KB
2015-01-14 19:28:08,219 INFO org.apache.hadoop.util.GSet: capacity      = 2^16 = 65536 entries
2015-01-14 19:28:08,230 INFO org.apache.hadoop.hdfs.server.namenode.NNConf: ACLs enabled? false
2015-01-14 19:28:08,230 INFO org.apache.hadoop.hdfs.server.namenode.NNConf: XAttrs enabled? true
2015-01-14 19:28:08,230 INFO org.apache.hadoop.hdfs.server.namenode.NNConf: Maximum size of an xattr: 16384
2015-01-14 19:28:08,277 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /tmp/hadoop-doopy/dfs/name/in_use.lock acquired by nodename 2480@barbie
2015-01-14 19:28:08,587 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Recovering unfinalized segments in /tmp/hadoop-doopy/dfs/name/current
2015-01-14 19:28:08,589 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: No edit log streams selected.
2015-01-14 19:28:08,837 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode: Loading 1 INodes.
2015-01-14 19:28:09,002 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Loaded FSImage in 0 seconds.
2015-01-14 19:28:09,002 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Loaded image for txid 0 from /tmp/hadoop-doopy/dfs/name/current/fsimage_0000000000000000000
2015-01-14 19:28:09,034 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Need to save fs image? false (staleImage=false, haEnabled=false, isRollingUpgrade=false)
2015-01-14 19:28:09,035 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 1
2015-01-14 19:28:09,546 INFO org.apache.hadoop.hdfs.server.namenode.NameCache: initialized with 0 entries 0 lookups
2015-01-14 19:28:09,546 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Finished loading FSImage in 1316 msecs
2015-01-14 19:28:12,989 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: RPC server is binding to localhost:9000
2015-01-14 19:28:13,055 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-01-14 19:28:13,164 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 9000
2015-01-14 19:28:13,999 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Registered FSNamesystemState MBean
2015-01-14 19:28:14,049 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Number of blocks under construction: 0
2015-01-14 19:28:14,050 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Number of blocks under construction: 0
2015-01-14 19:28:14,050 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: initializing replication queues
2015-01-14 19:28:14,050 INFO org.apache.hadoop.hdfs.StateChange: STATE* Leaving safe mode after 6 secs
2015-01-14 19:28:14,051 INFO org.apache.hadoop.hdfs.StateChange: STATE* Network topology has 0 racks and 0 datanodes
2015-01-14 19:28:14,051 INFO org.apache.hadoop.hdfs.StateChange: STATE* UnderReplicatedBlocks has 0 blocks
2015-01-14 19:28:14,202 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Total number of blocks            = 0
2015-01-14 19:28:14,202 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of invalid blocks          = 0
2015-01-14 19:28:14,202 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of under-replicated blocks = 0
2015-01-14 19:28:14,202 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of  over-replicated blocks = 0
2015-01-14 19:28:14,202 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of blocks being written    = 0
2015-01-14 19:28:14,202 INFO org.apache.hadoop.hdfs.StateChange: STATE* Replication Queue initialization scan for invalid, over- and under-replicated blocks completed in 142 msec
2015-01-14 19:28:14,326 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-01-14 19:28:14,388 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 9000: starting
2015-01-14 19:28:14,389 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: NameNode RPC up at: localhost/127.0.0.1:9000
2015-01-14 19:28:14,389 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Starting services required for active state
2015-01-14 19:28:14,434 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Starting CacheReplicationMonitor with interval 30000 milliseconds
2015-01-14 19:28:14,434 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 3062542 milliseconds
2015-01-14 19:28:14,456 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 9 millisecond(s).
2015-01-14 19:28:34,408 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(127.0.0.1, datanodeUuid=47972a9c-c7e0-400a-ad2b-3a4f63bb22be, infoPort=50075, ipcPort=50020, storageInfo=lv=-56;cid=CID-a0ed2779-c968-4c23-862a-f2f91301194e;nsid=131792437;c=0) storage 47972a9c-c7e0-400a-ad2b-3a4f63bb22be
2015-01-14 19:28:34,419 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2015-01-14 19:28:34,425 INFO org.apache.hadoop.net.NetworkTopology: Adding a new node: /default-rack/127.0.0.1:50010
2015-01-14 19:28:34,909 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2015-01-14 19:28:34,909 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Adding new storage ID DS-bc85abfe-cfe9-4d3b-8deb-497438f4cb62 for DN 127.0.0.1:50010
2015-01-14 19:28:35,098 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: BLOCK* processReport: Received first block report from DatanodeStorage[DS-bc85abfe-cfe9-4d3b-8deb-497438f4cb62,DISK,NORMAL] after starting up or becoming active. Its block contents are no longer considered stale
2015-01-14 19:28:35,098 INFO BlockStateChange: BLOCK* processReport: from storage DS-bc85abfe-cfe9-4d3b-8deb-497438f4cb62 node DatanodeRegistration(127.0.0.1, datanodeUuid=47972a9c-c7e0-400a-ad2b-3a4f63bb22be, infoPort=50075, ipcPort=50020, storageInfo=lv=-56;cid=CID-a0ed2779-c968-4c23-862a-f2f91301194e;nsid=131792437;c=0), blocks: 0, hasStaleStorages: false, processing time: 4 msecs
2015-01-14 19:28:44,447 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30013 milliseconds
2015-01-14 19:28:44,449 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 2 millisecond(s).
2015-01-14 19:29:08,842 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 3 Total time for transactions(ms): 29 Number of transactions batched in Syncs: 0 Number of syncs: 3 SyncTimes(ms): 175 
2015-01-14 19:29:14,448 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-01-14 19:29:14,450 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 3 millisecond(s).
2015-01-14 19:29:38,776 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /user/doopy/input/capacity-scheduler.xml._COPYING_. BP-1611003397-127.0.1.1-1421285133099 blk_1073741825_1001{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-bc85abfe-cfe9-4d3b-8deb-497438f4cb62:NORMAL:127.0.0.1:50010|RBW]]}
2015-01-14 19:29:39,482 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* checkFileProgress: blk_1073741825_1001{blockUCState=COMMITTED, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-bc85abfe-cfe9-4d3b-8deb-497438f4cb62:NORMAL:127.0.0.1:50010|RBW]]} has not reached minimal replication 1
2015-01-14 19:29:39,483 INFO org.apache.hadoop.hdfs.server.namenode.EditLogFileOutputStream: Nothing to flush
2015-01-14 19:29:39,548 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741825_1001{blockUCState=COMMITTED, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-bc85abfe-cfe9-4d3b-8deb-497438f4cb62:NORMAL:127.0.0.1:50010|RBW]]} size 4436
2015-01-14 19:29:39,931 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/doopy/input/capacity-scheduler.xml._COPYING_ is closed by DFSClient_NONMAPREDUCE_189475487_1
2015-01-14 19:29:40,024 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /user/doopy/input/configuration.xsl._COPYING_. BP-1611003397-127.0.1.1-1421285133099 blk_1073741826_1002{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-bc85abfe-cfe9-4d3b-8deb-497438f4cb62:NORMAL:127.0.0.1:50010|RBW]]}
2015-01-14 19:29:40,059 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741826_1002{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-bc85abfe-cfe9-4d3b-8deb-497438f4cb62:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-01-14 19:29:40,072 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/doopy/input/configuration.xsl._COPYING_ is closed by DFSClient_NONMAPREDUCE_189475487_1
2015-01-14 19:29:40,120 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /user/doopy/input/container-executor.cfg._COPYING_. BP-1611003397-127.0.1.1-1421285133099 blk_1073741827_1003{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-bc85abfe-cfe9-4d3b-8deb-497438f4cb62:NORMAL:127.0.0.1:50010|RBW]]}
2015-01-14 19:29:40,150 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741827_1003{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-bc85abfe-cfe9-4d3b-8deb-497438f4cb62:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-01-14 19:29:40,164 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/doopy/input/container-executor.cfg._COPYING_ is closed by DFSClient_NONMAPREDUCE_189475487_1
2015-01-14 19:29:40,196 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /user/doopy/input/core-site.xml._COPYING_. BP-1611003397-127.0.1.1-1421285133099 blk_1073741828_1004{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-bc85abfe-cfe9-4d3b-8deb-497438f4cb62:NORMAL:127.0.0.1:50010|RBW]]}
2015-01-14 19:29:40,217 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741828_1004{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-bc85abfe-cfe9-4d3b-8deb-497438f4cb62:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-01-14 19:29:40,230 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/doopy/input/core-site.xml._COPYING_ is closed by DFSClient_NONMAPREDUCE_189475487_1
2015-01-14 19:29:40,280 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /user/doopy/input/hadoop-env.cmd._COPYING_. BP-1611003397-127.0.1.1-1421285133099 blk_1073741829_1005{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-bc85abfe-cfe9-4d3b-8deb-497438f4cb62:NORMAL:127.0.0.1:50010|RBW]]}
2015-01-14 19:29:40,308 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741829_1005{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-bc85abfe-cfe9-4d3b-8deb-497438f4cb62:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-01-14 19:29:40,322 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/doopy/input/hadoop-env.cmd._COPYING_ is closed by DFSClient_NONMAPREDUCE_189475487_1
2015-01-14 19:29:40,353 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /user/doopy/input/hadoop-env.sh._COPYING_. BP-1611003397-127.0.1.1-1421285133099 blk_1073741830_1006{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-bc85abfe-cfe9-4d3b-8deb-497438f4cb62:NORMAL:127.0.0.1:50010|RBW]]}
2015-01-14 19:29:40,366 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741830_1006{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-bc85abfe-cfe9-4d3b-8deb-497438f4cb62:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-01-14 19:29:40,380 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/doopy/input/hadoop-env.sh._COPYING_ is closed by DFSClient_NONMAPREDUCE_189475487_1
2015-01-14 19:29:40,412 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /user/doopy/input/hadoop-metrics.properties._COPYING_. BP-1611003397-127.0.1.1-1421285133099 blk_1073741831_1007{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-bc85abfe-cfe9-4d3b-8deb-497438f4cb62:NORMAL:127.0.0.1:50010|RBW]]}
2015-01-14 19:29:40,425 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741831_1007{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-bc85abfe-cfe9-4d3b-8deb-497438f4cb62:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-01-14 19:29:40,439 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/doopy/input/hadoop-metrics.properties._COPYING_ is closed by DFSClient_NONMAPREDUCE_189475487_1
2015-01-14 19:29:40,472 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /user/doopy/input/hadoop-metrics2.properties._COPYING_. BP-1611003397-127.0.1.1-1421285133099 blk_1073741832_1008{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-bc85abfe-cfe9-4d3b-8deb-497438f4cb62:NORMAL:127.0.0.1:50010|RBW]]}
2015-01-14 19:29:40,497 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741832_1008{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-bc85abfe-cfe9-4d3b-8deb-497438f4cb62:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-01-14 19:29:40,505 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/doopy/input/hadoop-metrics2.properties._COPYING_ is closed by DFSClient_NONMAPREDUCE_189475487_1
2015-01-14 19:29:40,537 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /user/doopy/input/hadoop-policy.xml._COPYING_. BP-1611003397-127.0.1.1-1421285133099 blk_1073741833_1009{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-bc85abfe-cfe9-4d3b-8deb-497438f4cb62:NORMAL:127.0.0.1:50010|RBW]]}
2015-01-14 19:29:40,551 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741833_1009{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-bc85abfe-cfe9-4d3b-8deb-497438f4cb62:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-01-14 19:29:40,564 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/doopy/input/hadoop-policy.xml._COPYING_ is closed by DFSClient_NONMAPREDUCE_189475487_1
2015-01-14 19:29:40,595 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /user/doopy/input/hdfs-site.xml._COPYING_. BP-1611003397-127.0.1.1-1421285133099 blk_1073741834_1010{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-bc85abfe-cfe9-4d3b-8deb-497438f4cb62:NORMAL:127.0.0.1:50010|RBW]]}
2015-01-14 19:29:40,607 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741834_1010{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-bc85abfe-cfe9-4d3b-8deb-497438f4cb62:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-01-14 19:29:40,614 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/doopy/input/hdfs-site.xml._COPYING_ is closed by DFSClient_NONMAPREDUCE_189475487_1
2015-01-14 19:29:40,674 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /user/doopy/input/httpfs-env.sh._COPYING_. BP-1611003397-127.0.1.1-1421285133099 blk_1073741835_1011{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-bc85abfe-cfe9-4d3b-8deb-497438f4cb62:NORMAL:127.0.0.1:50010|RBW]]}
2015-01-14 19:29:40,710 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741835_1011{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-bc85abfe-cfe9-4d3b-8deb-497438f4cb62:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-01-14 19:29:40,722 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/doopy/input/httpfs-env.sh._COPYING_ is closed by DFSClient_NONMAPREDUCE_189475487_1
2015-01-14 19:29:40,773 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /user/doopy/input/httpfs-log4j.properties._COPYING_. BP-1611003397-127.0.1.1-1421285133099 blk_1073741836_1012{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-bc85abfe-cfe9-4d3b-8deb-497438f4cb62:NORMAL:127.0.0.1:50010|RBW]]}
2015-01-14 19:29:40,799 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741836_1012{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-bc85abfe-cfe9-4d3b-8deb-497438f4cb62:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-01-14 19:29:40,805 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/doopy/input/httpfs-log4j.properties._COPYING_ is closed by DFSClient_NONMAPREDUCE_189475487_1
2015-01-14 19:29:40,837 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /user/doopy/input/httpfs-signature.secret._COPYING_. BP-1611003397-127.0.1.1-1421285133099 blk_1073741837_1013{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-bc85abfe-cfe9-4d3b-8deb-497438f4cb62:NORMAL:127.0.0.1:50010|RBW]]}
2015-01-14 19:29:40,858 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741837_1013{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-bc85abfe-cfe9-4d3b-8deb-497438f4cb62:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-01-14 19:29:40,872 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/doopy/input/httpfs-signature.secret._COPYING_ is closed by DFSClient_NONMAPREDUCE_189475487_1
2015-01-14 19:29:40,905 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /user/doopy/input/httpfs-site.xml._COPYING_. BP-1611003397-127.0.1.1-1421285133099 blk_1073741838_1014{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-bc85abfe-cfe9-4d3b-8deb-497438f4cb62:NORMAL:127.0.0.1:50010|RBW]]}
2015-01-14 19:29:40,924 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741838_1014{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-bc85abfe-cfe9-4d3b-8deb-497438f4cb62:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-01-14 19:29:40,939 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/doopy/input/httpfs-site.xml._COPYING_ is closed by DFSClient_NONMAPREDUCE_189475487_1
2015-01-14 19:29:40,969 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /user/doopy/input/kms-acls.xml._COPYING_. BP-1611003397-127.0.1.1-1421285133099 blk_1073741839_1015{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-bc85abfe-cfe9-4d3b-8deb-497438f4cb62:NORMAL:127.0.0.1:50010|RBW]]}
2015-01-14 19:29:40,982 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741839_1015{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-bc85abfe-cfe9-4d3b-8deb-497438f4cb62:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-01-14 19:29:40,997 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/doopy/input/kms-acls.xml._COPYING_ is closed by DFSClient_NONMAPREDUCE_189475487_1
2015-01-14 19:29:41,055 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /user/doopy/input/kms-env.sh._COPYING_. BP-1611003397-127.0.1.1-1421285133099 blk_1073741840_1016{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-bc85abfe-cfe9-4d3b-8deb-497438f4cb62:NORMAL:127.0.0.1:50010|RBW]]}
2015-01-14 19:29:41,083 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741840_1016{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-bc85abfe-cfe9-4d3b-8deb-497438f4cb62:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-01-14 19:29:41,097 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/doopy/input/kms-env.sh._COPYING_ is closed by DFSClient_NONMAPREDUCE_189475487_1
2015-01-14 19:29:41,136 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /user/doopy/input/kms-log4j.properties._COPYING_. BP-1611003397-127.0.1.1-1421285133099 blk_1073741841_1017{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-bc85abfe-cfe9-4d3b-8deb-497438f4cb62:NORMAL:127.0.0.1:50010|RBW]]}
2015-01-14 19:29:41,148 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741841_1017{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-bc85abfe-cfe9-4d3b-8deb-497438f4cb62:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-01-14 19:29:41,155 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/doopy/input/kms-log4j.properties._COPYING_ is closed by DFSClient_NONMAPREDUCE_189475487_1
2015-01-14 19:29:41,186 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /user/doopy/input/kms-site.xml._COPYING_. BP-1611003397-127.0.1.1-1421285133099 blk_1073741842_1018{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-bc85abfe-cfe9-4d3b-8deb-497438f4cb62:NORMAL:127.0.0.1:50010|RBW]]}
2015-01-14 19:29:41,200 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741842_1018{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-bc85abfe-cfe9-4d3b-8deb-497438f4cb62:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-01-14 19:29:41,213 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/doopy/input/kms-site.xml._COPYING_ is closed by DFSClient_NONMAPREDUCE_189475487_1
2015-01-14 19:29:41,244 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /user/doopy/input/log4j.properties._COPYING_. BP-1611003397-127.0.1.1-1421285133099 blk_1073741843_1019{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-bc85abfe-cfe9-4d3b-8deb-497438f4cb62:NORMAL:127.0.0.1:50010|RBW]]}
2015-01-14 19:29:41,259 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741843_1019{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-bc85abfe-cfe9-4d3b-8deb-497438f4cb62:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-01-14 19:29:41,272 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/doopy/input/log4j.properties._COPYING_ is closed by DFSClient_NONMAPREDUCE_189475487_1
2015-01-14 19:29:41,320 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /user/doopy/input/mapred-env.cmd._COPYING_. BP-1611003397-127.0.1.1-1421285133099 blk_1073741844_1020{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-bc85abfe-cfe9-4d3b-8deb-497438f4cb62:NORMAL:127.0.0.1:50010|RBW]]}
2015-01-14 19:29:41,349 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741844_1020{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-bc85abfe-cfe9-4d3b-8deb-497438f4cb62:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-01-14 19:29:41,355 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/doopy/input/mapred-env.cmd._COPYING_ is closed by DFSClient_NONMAPREDUCE_189475487_1
2015-01-14 19:29:41,390 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /user/doopy/input/mapred-env.sh._COPYING_. BP-1611003397-127.0.1.1-1421285133099 blk_1073741845_1021{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-bc85abfe-cfe9-4d3b-8deb-497438f4cb62:NORMAL:127.0.0.1:50010|RBW]]}
2015-01-14 19:29:41,406 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741845_1021{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-bc85abfe-cfe9-4d3b-8deb-497438f4cb62:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-01-14 19:29:41,413 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/doopy/input/mapred-env.sh._COPYING_ is closed by DFSClient_NONMAPREDUCE_189475487_1
2015-01-14 19:29:41,457 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /user/doopy/input/mapred-queues.xml.template._COPYING_. BP-1611003397-127.0.1.1-1421285133099 blk_1073741846_1022{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-bc85abfe-cfe9-4d3b-8deb-497438f4cb62:NORMAL:127.0.0.1:50010|RBW]]}
2015-01-14 19:29:41,482 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741846_1022{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-bc85abfe-cfe9-4d3b-8deb-497438f4cb62:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-01-14 19:29:41,497 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/doopy/input/mapred-queues.xml.template._COPYING_ is closed by DFSClient_NONMAPREDUCE_189475487_1
2015-01-14 19:29:41,541 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /user/doopy/input/mapred-site.xml.template._COPYING_. BP-1611003397-127.0.1.1-1421285133099 blk_1073741847_1023{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-bc85abfe-cfe9-4d3b-8deb-497438f4cb62:NORMAL:127.0.0.1:50010|RBW]]}
2015-01-14 19:29:41,578 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741847_1023{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-bc85abfe-cfe9-4d3b-8deb-497438f4cb62:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-01-14 19:29:41,588 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/doopy/input/mapred-site.xml.template._COPYING_ is closed by DFSClient_NONMAPREDUCE_189475487_1
2015-01-14 19:29:41,619 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /user/doopy/input/slaves._COPYING_. BP-1611003397-127.0.1.1-1421285133099 blk_1073741848_1024{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-bc85abfe-cfe9-4d3b-8deb-497438f4cb62:NORMAL:127.0.0.1:50010|RBW]]}
2015-01-14 19:29:41,631 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741848_1024{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-bc85abfe-cfe9-4d3b-8deb-497438f4cb62:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-01-14 19:29:41,638 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/doopy/input/slaves._COPYING_ is closed by DFSClient_NONMAPREDUCE_189475487_1
2015-01-14 19:29:41,688 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /user/doopy/input/ssl-client.xml.example._COPYING_. BP-1611003397-127.0.1.1-1421285133099 blk_1073741849_1025{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-bc85abfe-cfe9-4d3b-8deb-497438f4cb62:NORMAL:127.0.0.1:50010|RBW]]}
2015-01-14 19:29:41,717 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741849_1025{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-bc85abfe-cfe9-4d3b-8deb-497438f4cb62:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-01-14 19:29:41,730 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/doopy/input/ssl-client.xml.example._COPYING_ is closed by DFSClient_NONMAPREDUCE_189475487_1
2015-01-14 19:29:41,761 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /user/doopy/input/ssl-server.xml.example._COPYING_. BP-1611003397-127.0.1.1-1421285133099 blk_1073741850_1026{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-bc85abfe-cfe9-4d3b-8deb-497438f4cb62:NORMAL:127.0.0.1:50010|RBW]]}
2015-01-14 19:29:41,773 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741850_1026{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-bc85abfe-cfe9-4d3b-8deb-497438f4cb62:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-01-14 19:29:41,780 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/doopy/input/ssl-server.xml.example._COPYING_ is closed by DFSClient_NONMAPREDUCE_189475487_1
2015-01-14 19:29:41,811 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /user/doopy/input/yarn-env.cmd._COPYING_. BP-1611003397-127.0.1.1-1421285133099 blk_1073741851_1027{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-bc85abfe-cfe9-4d3b-8deb-497438f4cb62:NORMAL:127.0.0.1:50010|RBW]]}
2015-01-14 19:29:41,824 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741851_1027{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-bc85abfe-cfe9-4d3b-8deb-497438f4cb62:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-01-14 19:29:41,838 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/doopy/input/yarn-env.cmd._COPYING_ is closed by DFSClient_NONMAPREDUCE_189475487_1
2015-01-14 19:29:41,882 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /user/doopy/input/yarn-env.sh._COPYING_. BP-1611003397-127.0.1.1-1421285133099 blk_1073741852_1028{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-bc85abfe-cfe9-4d3b-8deb-497438f4cb62:NORMAL:127.0.0.1:50010|RBW]]}
2015-01-14 19:29:41,906 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741852_1028{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-bc85abfe-cfe9-4d3b-8deb-497438f4cb62:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-01-14 19:29:41,913 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/doopy/input/yarn-env.sh._COPYING_ is closed by DFSClient_NONMAPREDUCE_189475487_1
2015-01-14 19:29:41,944 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /user/doopy/input/yarn-site.xml._COPYING_. BP-1611003397-127.0.1.1-1421285133099 blk_1073741853_1029{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-bc85abfe-cfe9-4d3b-8deb-497438f4cb62:NORMAL:127.0.0.1:50010|RBW]]}
2015-01-14 19:29:41,957 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741853_1029{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-bc85abfe-cfe9-4d3b-8deb-497438f4cb62:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-01-14 19:29:41,963 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/doopy/input/yarn-site.xml._COPYING_ is closed by DFSClient_NONMAPREDUCE_189475487_1
2015-01-14 19:29:44,447 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-01-14 19:29:44,449 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 2 millisecond(s).
2015-01-14 19:30:14,447 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-01-14 19:30:14,450 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 2 millisecond(s).
2015-01-14 19:30:31,587 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 181 Total time for transactions(ms): 43 Number of transactions batched in Syncs: 0 Number of syncs: 122 SyncTimes(ms): 1031 
2015-01-14 19:30:39,726 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /user/doopy/grep-temp-778403726/_temporary/0/_temporary/attempt_local1939389613_0001_r_000000_0/part-r-00000. BP-1611003397-127.0.1.1-1421285133099 blk_1073741854_1030{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-bc85abfe-cfe9-4d3b-8deb-497438f4cb62:NORMAL:127.0.0.1:50010|RBW]]}
2015-01-14 19:30:39,912 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741854_1030{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-bc85abfe-cfe9-4d3b-8deb-497438f4cb62:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-01-14 19:30:39,935 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/doopy/grep-temp-778403726/_temporary/0/_temporary/attempt_local1939389613_0001_r_000000_0/part-r-00000 is closed by DFSClient_NONMAPREDUCE_-1954089522_1
2015-01-14 19:30:40,093 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/doopy/grep-temp-778403726/_SUCCESS is closed by DFSClient_NONMAPREDUCE_-1954089522_1
2015-01-14 19:30:41,617 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /user/doopy/output/_temporary/0/_temporary/attempt_local970733892_0002_r_000000_0/part-r-00000. BP-1611003397-127.0.1.1-1421285133099 blk_1073741855_1031{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-bc85abfe-cfe9-4d3b-8deb-497438f4cb62:NORMAL:127.0.0.1:50010|RBW]]}
2015-01-14 19:30:41,637 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741855_1031{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-bc85abfe-cfe9-4d3b-8deb-497438f4cb62:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-01-14 19:30:41,651 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/doopy/output/_temporary/0/_temporary/attempt_local970733892_0002_r_000000_0/part-r-00000 is closed by DFSClient_NONMAPREDUCE_-1954089522_1
2015-01-14 19:30:41,734 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/doopy/output/_SUCCESS is closed by DFSClient_NONMAPREDUCE_-1954089522_1
2015-01-14 19:30:42,457 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741854_1030 127.0.0.1:50010 
2015-01-14 19:30:44,117 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* BlockManager: ask 127.0.0.1:50010 to delete [blk_1073741854_1030]
2015-01-14 19:30:44,448 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-01-14 19:30:44,450 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 2 millisecond(s).
2015-01-14 19:31:14,448 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-01-14 19:31:14,451 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 2 millisecond(s).
2015-01-14 19:31:44,449 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-01-14 19:31:44,451 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 2 millisecond(s).
2015-01-14 19:31:58,580 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: RECEIVED SIGNAL 15: SIGTERM
2015-01-14 19:31:58,582 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down NameNode at barbie/127.0.1.1
************************************************************/
2015-01-14 19:39:18,064 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting NameNode
STARTUP_MSG:   host = barbie/127.0.1.1
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.6.0
STARTUP_MSG:   classpath = /usr/local/hadoop-2.6.0/etc/hadoop:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/commons-collections-3.2.1.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/hadoop-annotations-2.6.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/jasper-runtime-5.5.23.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/curator-client-2.6.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/curator-framework-2.6.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/jsr305-1.3.9.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/curator-recipes-2.6.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/htrace-core-3.0.4.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/commons-el-1.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/jasper-compiler-5.5.23.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/hadoop-auth-2.6.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/slf4j-api-1.7.5.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/hadoop-common-2.6.0-tests.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/hadoop-common-2.6.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/hadoop-nfs-2.6.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/hdfs:/usr/local/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop-2.6.0/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.6.0/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop-2.6.0/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.6.0/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.6.0/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop-2.6.0/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.0/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.0/share/hadoop/hdfs/lib/jasper-runtime-5.5.23.jar:/usr/local/hadoop-2.6.0/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop-2.6.0/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.6.0/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.6.0/share/hadoop/hdfs/lib/jsr305-1.3.9.jar:/usr/local/hadoop-2.6.0/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop-2.6.0/share/hadoop/hdfs/lib/htrace-core-3.0.4.jar:/usr/local/hadoop-2.6.0/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-el-1.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.0/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.6.0/share/hadoop/hdfs/lib/jsp-api-2.1.jar:/usr/local/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-2.6.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-2.6.0-tests.jar:/usr/local/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-nfs-2.6.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/lib/commons-httpclient-3.1.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/lib/jsr305-1.3.9.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/lib/jline-0.9.94.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-registry-2.6.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.6.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-tests-2.6.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.6.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-client-2.6.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.6.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-common-2.6.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.6.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-common-2.6.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.6.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-api-2.6.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.6.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/mapreduce/lib/hadoop-annotations-2.6.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop-2.6.0/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.0/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop-2.6.0/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop-2.6.0/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.0/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.0/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop-2.6.0/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop-2.6.0/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop-2.6.0/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop-2.6.0/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.0/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop-2.6.0/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.6.0/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.0/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.0/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.6.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.6.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.6.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.6.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.6.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.0-tests.jar:/usr/local/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.6.0.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://github.com/apache/hadoop -r e3496499ecb8d220fba99dc5ed4c99c8f9e33bb1; compiled by 'doopy' on 2015-01-14T23:00Z
STARTUP_MSG:   java = 1.7.0_65
************************************************************/
2015-01-14 19:39:18,087 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-01-14 19:39:18,094 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: createNameNode []
2015-01-14 19:39:18,978 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-01-14 19:39:19,228 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-01-14 19:39:19,228 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: NameNode metrics system started
2015-01-14 19:39:19,231 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: fs.defaultFS is hdfs://localhost:9000
2015-01-14 19:39:19,231 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Clients are to use localhost:9000 to access this namenode/service.
2015-01-14 19:39:19,620 INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for hdfs at: http://0.0.0.0:50070
2015-01-14 19:39:19,725 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-01-14 19:39:19,733 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.namenode is not defined
2015-01-14 19:39:19,754 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-01-14 19:39:19,762 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs
2015-01-14 19:39:19,762 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-01-14 19:39:19,762 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-01-14 19:39:19,824 INFO org.apache.hadoop.http.HttpServer2: Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)
2015-01-14 19:39:19,827 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2015-01-14 19:39:19,862 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50070
2015-01-14 19:39:19,862 INFO org.mortbay.log: jetty-6.1.26
2015-01-14 19:39:20,356 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50070
2015-01-14 19:39:20,437 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one image storage directory (dfs.namenode.name.dir) configured. Beware of data loss due to lack of redundant storage directories!
2015-01-14 19:39:20,437 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one namespace edits storage directory (dfs.namenode.edits.dir) configured. Beware of data loss due to lack of redundant storage directories!
2015-01-14 19:39:20,525 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: No KeyProvider found.
2015-01-14 19:39:20,549 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair:true
2015-01-14 19:39:20,630 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit=1000
2015-01-14 19:39:20,630 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
2015-01-14 19:39:20,638 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2015-01-14 19:39:20,640 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: The block deletion will start around 2015 Jan 14 19:39:20
2015-01-14 19:39:20,644 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlocksMap
2015-01-14 19:39:20,644 INFO org.apache.hadoop.util.GSet: VM type       = 32-bit
2015-01-14 19:39:20,648 INFO org.apache.hadoop.util.GSet: 2.0% max memory 966.7 MB = 19.3 MB
2015-01-14 19:39:20,648 INFO org.apache.hadoop.util.GSet: capacity      = 2^22 = 4194304 entries
2015-01-14 19:39:20,735 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable=false
2015-01-14 19:39:20,736 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: defaultReplication         = 1
2015-01-14 19:39:20,736 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplication             = 512
2015-01-14 19:39:20,736 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: minReplication             = 1
2015-01-14 19:39:20,736 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplicationStreams      = 2
2015-01-14 19:39:20,736 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: shouldCheckForEnoughRacks  = false
2015-01-14 19:39:20,736 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: replicationRecheckInterval = 3000
2015-01-14 19:39:20,736 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: encryptDataTransfer        = false
2015-01-14 19:39:20,736 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
2015-01-14 19:39:20,749 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             = doopy (auth:SIMPLE)
2015-01-14 19:39:20,749 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          = supergroup
2015-01-14 19:39:20,749 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled = true
2015-01-14 19:39:20,750 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled: false
2015-01-14 19:39:20,753 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Append Enabled: true
2015-01-14 19:39:20,855 INFO org.apache.hadoop.util.GSet: Computing capacity for map INodeMap
2015-01-14 19:39:20,855 INFO org.apache.hadoop.util.GSet: VM type       = 32-bit
2015-01-14 19:39:20,856 INFO org.apache.hadoop.util.GSet: 1.0% max memory 966.7 MB = 9.7 MB
2015-01-14 19:39:20,856 INFO org.apache.hadoop.util.GSet: capacity      = 2^21 = 2097152 entries
2015-01-14 19:39:20,943 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Caching file names occuring more than 10 times
2015-01-14 19:39:20,960 INFO org.apache.hadoop.util.GSet: Computing capacity for map cachedBlocks
2015-01-14 19:39:20,960 INFO org.apache.hadoop.util.GSet: VM type       = 32-bit
2015-01-14 19:39:20,960 INFO org.apache.hadoop.util.GSet: 0.25% max memory 966.7 MB = 2.4 MB
2015-01-14 19:39:20,961 INFO org.apache.hadoop.util.GSet: capacity      = 2^19 = 524288 entries
2015-01-14 19:39:20,965 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2015-01-14 19:39:20,965 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.min.datanodes = 0
2015-01-14 19:39:20,965 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.extension     = 30000
2015-01-14 19:39:20,967 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache on namenode is enabled
2015-01-14 19:39:20,968 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2015-01-14 19:39:20,972 INFO org.apache.hadoop.util.GSet: Computing capacity for map NameNodeRetryCache
2015-01-14 19:39:20,972 INFO org.apache.hadoop.util.GSet: VM type       = 32-bit
2015-01-14 19:39:20,973 INFO org.apache.hadoop.util.GSet: 0.029999999329447746% max memory 966.7 MB = 297.0 KB
2015-01-14 19:39:20,973 INFO org.apache.hadoop.util.GSet: capacity      = 2^16 = 65536 entries
2015-01-14 19:39:20,984 INFO org.apache.hadoop.hdfs.server.namenode.NNConf: ACLs enabled? false
2015-01-14 19:39:20,984 INFO org.apache.hadoop.hdfs.server.namenode.NNConf: XAttrs enabled? true
2015-01-14 19:39:20,984 INFO org.apache.hadoop.hdfs.server.namenode.NNConf: Maximum size of an xattr: 16384
2015-01-14 19:39:21,026 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /tmp/hadoop-doopy/dfs/name/in_use.lock acquired by nodename 3685@barbie
2015-01-14 19:39:21,187 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Recovering unfinalized segments in /tmp/hadoop-doopy/dfs/name/current
2015-01-14 19:39:21,571 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /tmp/hadoop-doopy/dfs/name/current/edits_inprogress_0000000000000000001 -> /tmp/hadoop-doopy/dfs/name/current/edits_0000000000000000001-0000000000000000209
2015-01-14 19:39:21,707 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode: Loading 1 INodes.
2015-01-14 19:39:21,861 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Loaded FSImage in 0 seconds.
2015-01-14 19:39:21,862 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Loaded image for txid 0 from /tmp/hadoop-doopy/dfs/name/current/fsimage_0000000000000000000
2015-01-14 19:39:21,862 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@7c1c14 expecting start txid #1
2015-01-14 19:39:21,862 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file /tmp/hadoop-doopy/dfs/name/current/edits_0000000000000000001-0000000000000000209
2015-01-14 19:39:21,877 INFO org.apache.hadoop.hdfs.server.namenode.EditLogInputStream: Fast-forwarding stream '/tmp/hadoop-doopy/dfs/name/current/edits_0000000000000000001-0000000000000000209' to transaction ID 1
2015-01-14 19:39:22,116 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file /tmp/hadoop-doopy/dfs/name/current/edits_0000000000000000001-0000000000000000209 of size 1048576 edits # 209 loaded in 0 seconds
2015-01-14 19:39:22,117 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Need to save fs image? false (staleImage=false, haEnabled=false, isRollingUpgrade=false)
2015-01-14 19:39:22,118 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 210
2015-01-14 19:39:22,396 INFO org.apache.hadoop.hdfs.server.namenode.NameCache: initialized with 0 entries 0 lookups
2015-01-14 19:39:22,396 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Finished loading FSImage in 1412 msecs
2015-01-14 19:39:23,177 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: RPC server is binding to localhost:9000
2015-01-14 19:39:23,206 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-01-14 19:39:23,273 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 9000
2015-01-14 19:39:23,514 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Registered FSNamesystemState MBean
2015-01-14 19:39:23,571 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Number of blocks under construction: 0
2015-01-14 19:39:23,572 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Number of blocks under construction: 0
2015-01-14 19:39:23,577 INFO org.apache.hadoop.hdfs.StateChange: STATE* Safe mode ON. 
The reported blocks 0 needs additional 30 blocks to reach the threshold 0.9990 of total blocks 30.
The number of live datanodes 0 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached.
2015-01-14 19:39:23,699 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-01-14 19:39:23,714 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: NameNode RPC up at: localhost/127.0.0.1:9000
2015-01-14 19:39:23,715 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Starting services required for active state
2015-01-14 19:39:23,720 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 9000: starting
2015-01-14 19:39:23,752 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Starting CacheReplicationMonitor with interval 30000 milliseconds
2015-01-14 19:39:23,752 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 3731860 milliseconds
2015-01-14 19:39:23,763 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 11 millisecond(s).
2015-01-14 19:39:42,803 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(127.0.0.1, datanodeUuid=47972a9c-c7e0-400a-ad2b-3a4f63bb22be, infoPort=50075, ipcPort=50020, storageInfo=lv=-56;cid=CID-a0ed2779-c968-4c23-862a-f2f91301194e;nsid=131792437;c=0) storage 47972a9c-c7e0-400a-ad2b-3a4f63bb22be
2015-01-14 19:39:42,824 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2015-01-14 19:39:42,826 INFO org.apache.hadoop.net.NetworkTopology: Adding a new node: /default-rack/127.0.0.1:50010
2015-01-14 19:39:43,706 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2015-01-14 19:39:43,712 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Adding new storage ID DS-bc85abfe-cfe9-4d3b-8deb-497438f4cb62 for DN 127.0.0.1:50010
2015-01-14 19:39:44,078 INFO org.apache.hadoop.hdfs.StateChange: STATE* Safe mode ON. 
The reported blocks 1 needs additional 29 blocks to reach the threshold 0.9990 of total blocks 30.
The number of live datanodes 1 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached.
2015-01-14 19:39:44,093 INFO org.apache.hadoop.hdfs.StateChange: STATE* Safe mode extension entered. 
The reported blocks 29 has reached the threshold 0.9990 of total blocks 30. The number of live datanodes 1 has reached the minimum number 0. In safe mode extension. Safe mode will be turned off automatically in 29 seconds.
2015-01-14 19:39:44,093 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: initializing replication queues
2015-01-14 19:39:44,097 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: BLOCK* processReport: Received first block report from DatanodeStorage[DS-bc85abfe-cfe9-4d3b-8deb-497438f4cb62,DISK,NORMAL] after starting up or becoming active. Its block contents are no longer considered stale
2015-01-14 19:39:44,114 INFO BlockStateChange: BLOCK* processReport: from storage DS-bc85abfe-cfe9-4d3b-8deb-497438f4cb62 node DatanodeRegistration(127.0.0.1, datanodeUuid=47972a9c-c7e0-400a-ad2b-3a4f63bb22be, infoPort=50075, ipcPort=50020, storageInfo=lv=-56;cid=CID-a0ed2779-c968-4c23-862a-f2f91301194e;nsid=131792437;c=0), blocks: 30, hasStaleStorages: false, processing time: 33 msecs
2015-01-14 19:39:44,204 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Total number of blocks            = 30
2015-01-14 19:39:44,204 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of invalid blocks          = 0
2015-01-14 19:39:44,204 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of under-replicated blocks = 0
2015-01-14 19:39:44,204 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of  over-replicated blocks = 0
2015-01-14 19:39:44,204 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of blocks being written    = 0
2015-01-14 19:39:44,204 INFO org.apache.hadoop.hdfs.StateChange: STATE* Replication Queue initialization scan for invalid, over- and under-replicated blocks completed in 104 msec
2015-01-14 19:39:53,752 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-01-14 19:39:53,755 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 2 millisecond(s).
2015-01-14 19:40:04,106 INFO org.apache.hadoop.hdfs.StateChange: STATE* Safe mode ON, in safe mode extension. 
The reported blocks 30 has reached the threshold 0.9990 of total blocks 30. The number of live datanodes 1 has reached the minimum number 0. In safe mode extension. Safe mode will be turned off automatically in 9 seconds.
2015-01-14 19:40:14,107 INFO org.apache.hadoop.hdfs.StateChange: STATE* Leaving safe mode after 53 secs
2015-01-14 19:40:14,108 INFO org.apache.hadoop.hdfs.StateChange: STATE* Safe mode is OFF
2015-01-14 19:40:14,108 INFO org.apache.hadoop.hdfs.StateChange: STATE* Network topology has 1 racks and 1 datanodes
2015-01-14 19:40:14,108 INFO org.apache.hadoop.hdfs.StateChange: STATE* UnderReplicatedBlocks has 0 blocks
2015-01-14 19:40:23,753 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-01-14 19:40:23,755 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 2 millisecond(s).
2015-01-14 19:40:40,848 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Roll Edit Log from 127.0.0.1
2015-01-14 19:40:40,848 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Rolling edit logs
2015-01-14 19:40:40,848 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Ending log segment 210
2015-01-14 19:40:40,849 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 2 Number of transactions batched in Syncs: 0 Number of syncs: 2 SyncTimes(ms): 96 
2015-01-14 19:40:40,866 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 2 Number of transactions batched in Syncs: 0 Number of syncs: 3 SyncTimes(ms): 113 
2015-01-14 19:40:40,868 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /tmp/hadoop-doopy/dfs/name/current/edits_inprogress_0000000000000000210 -> /tmp/hadoop-doopy/dfs/name/current/edits_0000000000000000210-0000000000000000211
2015-01-14 19:40:40,868 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 212
2015-01-14 19:40:43,740 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.04s at 55.56 KB/s
2015-01-14 19:40:43,741 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file fsimage.ckpt_0000000000000000211 size 2983 bytes.
2015-01-14 19:40:43,776 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 0
2015-01-14 19:40:53,753 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-01-14 19:40:53,755 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 2 millisecond(s).
2015-01-14 19:41:23,754 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-01-14 19:41:23,756 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 3 millisecond(s).
2015-01-14 19:41:53,753 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-01-14 19:41:53,755 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 2 millisecond(s).
2015-01-14 19:42:11,394 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: RECEIVED SIGNAL 15: SIGTERM
2015-01-14 19:42:11,396 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down NameNode at barbie/127.0.1.1
************************************************************/
2015-01-14 19:46:05,160 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting NameNode
STARTUP_MSG:   host = barbie/127.0.1.1
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.6.0
STARTUP_MSG:   classpath = /usr/local/hadoop-2.6.0/etc/hadoop:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/commons-collections-3.2.1.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/hadoop-annotations-2.6.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/jasper-runtime-5.5.23.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/curator-client-2.6.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/curator-framework-2.6.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/jsr305-1.3.9.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/curator-recipes-2.6.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/htrace-core-3.0.4.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/commons-el-1.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/jasper-compiler-5.5.23.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/hadoop-auth-2.6.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/slf4j-api-1.7.5.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/hadoop-common-2.6.0-tests.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/hadoop-common-2.6.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/common/hadoop-nfs-2.6.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/hdfs:/usr/local/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop-2.6.0/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.6.0/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop-2.6.0/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.6.0/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.6.0/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop-2.6.0/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.0/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.0/share/hadoop/hdfs/lib/jasper-runtime-5.5.23.jar:/usr/local/hadoop-2.6.0/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop-2.6.0/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.6.0/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.6.0/share/hadoop/hdfs/lib/jsr305-1.3.9.jar:/usr/local/hadoop-2.6.0/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop-2.6.0/share/hadoop/hdfs/lib/htrace-core-3.0.4.jar:/usr/local/hadoop-2.6.0/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-el-1.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.0/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.0/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.6.0/share/hadoop/hdfs/lib/jsp-api-2.1.jar:/usr/local/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-2.6.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-2.6.0-tests.jar:/usr/local/hadoop-2.6.0/share/hadoop/hdfs/hadoop-hdfs-nfs-2.6.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/lib/commons-httpclient-3.1.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/lib/jsr305-1.3.9.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/lib/jline-0.9.94.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-registry-2.6.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.6.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-tests-2.6.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.6.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-client-2.6.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.6.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-common-2.6.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.6.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-common-2.6.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.6.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-api-2.6.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.6.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/mapreduce/lib/hadoop-annotations-2.6.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop-2.6.0/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.6.0/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop-2.6.0/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop-2.6.0/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.6.0/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop-2.6.0/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop-2.6.0/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop-2.6.0/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop-2.6.0/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop-2.6.0/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.6.0/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop-2.6.0/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.6.0/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.6.0/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.6.0/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.6.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.6.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.6.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.6.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.6.0.jar:/usr/local/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.6.0-tests.jar:/usr/local/hadoop-2.6.0/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.6.0.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://github.com/apache/hadoop -r e3496499ecb8d220fba99dc5ed4c99c8f9e33bb1; compiled by 'doopy' on 2015-01-14T23:00Z
STARTUP_MSG:   java = 1.7.0_65
************************************************************/
2015-01-14 19:46:05,183 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-01-14 19:46:05,189 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: createNameNode []
2015-01-14 19:46:06,064 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-01-14 19:46:06,313 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-01-14 19:46:06,313 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: NameNode metrics system started
2015-01-14 19:46:06,316 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: fs.defaultFS is hdfs://localhost:9000
2015-01-14 19:46:06,317 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Clients are to use localhost:9000 to access this namenode/service.
2015-01-14 19:46:06,704 INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for hdfs at: http://0.0.0.0:50070
2015-01-14 19:46:06,810 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-01-14 19:46:06,818 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.namenode is not defined
2015-01-14 19:46:06,839 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-01-14 19:46:06,847 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs
2015-01-14 19:46:06,847 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-01-14 19:46:06,847 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-01-14 19:46:06,909 INFO org.apache.hadoop.http.HttpServer2: Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)
2015-01-14 19:46:06,912 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2015-01-14 19:46:06,947 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50070
2015-01-14 19:46:06,947 INFO org.mortbay.log: jetty-6.1.26
2015-01-14 19:46:07,419 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50070
2015-01-14 19:46:07,498 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one image storage directory (dfs.namenode.name.dir) configured. Beware of data loss due to lack of redundant storage directories!
2015-01-14 19:46:07,498 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one namespace edits storage directory (dfs.namenode.edits.dir) configured. Beware of data loss due to lack of redundant storage directories!
2015-01-14 19:46:07,588 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: No KeyProvider found.
2015-01-14 19:46:07,612 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair:true
2015-01-14 19:46:07,693 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit=1000
2015-01-14 19:46:07,693 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
2015-01-14 19:46:07,701 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2015-01-14 19:46:07,703 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: The block deletion will start around 2015 Jan 14 19:46:07
2015-01-14 19:46:07,707 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlocksMap
2015-01-14 19:46:07,707 INFO org.apache.hadoop.util.GSet: VM type       = 32-bit
2015-01-14 19:46:07,711 INFO org.apache.hadoop.util.GSet: 2.0% max memory 966.7 MB = 19.3 MB
2015-01-14 19:46:07,711 INFO org.apache.hadoop.util.GSet: capacity      = 2^22 = 4194304 entries
2015-01-14 19:46:07,798 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable=false
2015-01-14 19:46:07,799 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: defaultReplication         = 1
2015-01-14 19:46:07,799 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplication             = 512
2015-01-14 19:46:07,799 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: minReplication             = 1
2015-01-14 19:46:07,799 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplicationStreams      = 2
2015-01-14 19:46:07,799 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: shouldCheckForEnoughRacks  = false
2015-01-14 19:46:07,799 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: replicationRecheckInterval = 3000
2015-01-14 19:46:07,799 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: encryptDataTransfer        = false
2015-01-14 19:46:07,799 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
2015-01-14 19:46:07,812 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             = doopy (auth:SIMPLE)
2015-01-14 19:46:07,812 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          = supergroup
2015-01-14 19:46:07,812 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled = true
2015-01-14 19:46:07,812 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled: false
2015-01-14 19:46:07,817 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Append Enabled: true
2015-01-14 19:46:07,918 INFO org.apache.hadoop.util.GSet: Computing capacity for map INodeMap
2015-01-14 19:46:07,918 INFO org.apache.hadoop.util.GSet: VM type       = 32-bit
2015-01-14 19:46:07,919 INFO org.apache.hadoop.util.GSet: 1.0% max memory 966.7 MB = 9.7 MB
2015-01-14 19:46:07,919 INFO org.apache.hadoop.util.GSet: capacity      = 2^21 = 2097152 entries
2015-01-14 19:46:08,006 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Caching file names occuring more than 10 times
2015-01-14 19:46:08,023 INFO org.apache.hadoop.util.GSet: Computing capacity for map cachedBlocks
2015-01-14 19:46:08,024 INFO org.apache.hadoop.util.GSet: VM type       = 32-bit
2015-01-14 19:46:08,024 INFO org.apache.hadoop.util.GSet: 0.25% max memory 966.7 MB = 2.4 MB
2015-01-14 19:46:08,024 INFO org.apache.hadoop.util.GSet: capacity      = 2^19 = 524288 entries
2015-01-14 19:46:08,029 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2015-01-14 19:46:08,029 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.min.datanodes = 0
2015-01-14 19:46:08,029 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.extension     = 30000
2015-01-14 19:46:08,031 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache on namenode is enabled
2015-01-14 19:46:08,031 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2015-01-14 19:46:08,036 INFO org.apache.hadoop.util.GSet: Computing capacity for map NameNodeRetryCache
2015-01-14 19:46:08,036 INFO org.apache.hadoop.util.GSet: VM type       = 32-bit
2015-01-14 19:46:08,036 INFO org.apache.hadoop.util.GSet: 0.029999999329447746% max memory 966.7 MB = 297.0 KB
2015-01-14 19:46:08,036 INFO org.apache.hadoop.util.GSet: capacity      = 2^16 = 65536 entries
2015-01-14 19:46:08,047 INFO org.apache.hadoop.hdfs.server.namenode.NNConf: ACLs enabled? false
2015-01-14 19:46:08,047 INFO org.apache.hadoop.hdfs.server.namenode.NNConf: XAttrs enabled? true
2015-01-14 19:46:08,048 INFO org.apache.hadoop.hdfs.server.namenode.NNConf: Maximum size of an xattr: 16384
2015-01-14 19:46:08,086 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /tmp/hadoop-doopy/dfs/name/in_use.lock acquired by nodename 4524@barbie
2015-01-14 19:46:08,248 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Recovering unfinalized segments in /tmp/hadoop-doopy/dfs/name/current
2015-01-14 19:46:08,334 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /tmp/hadoop-doopy/dfs/name/current/edits_inprogress_0000000000000000212 -> /tmp/hadoop-doopy/dfs/name/current/edits_0000000000000000212-0000000000000000212
2015-01-14 19:46:08,529 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode: Loading 36 INodes.
2015-01-14 19:46:08,779 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Loaded FSImage in 0 seconds.
2015-01-14 19:46:08,784 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Loaded image for txid 211 from /tmp/hadoop-doopy/dfs/name/current/fsimage_0000000000000000211
2015-01-14 19:46:08,784 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@1903e34 expecting start txid #212
2015-01-14 19:46:08,785 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file /tmp/hadoop-doopy/dfs/name/current/edits_0000000000000000212-0000000000000000212
2015-01-14 19:46:08,797 INFO org.apache.hadoop.hdfs.server.namenode.EditLogInputStream: Fast-forwarding stream '/tmp/hadoop-doopy/dfs/name/current/edits_0000000000000000212-0000000000000000212' to transaction ID 212
2015-01-14 19:46:08,808 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file /tmp/hadoop-doopy/dfs/name/current/edits_0000000000000000212-0000000000000000212 of size 1048576 edits # 1 loaded in 0 seconds
2015-01-14 19:46:08,820 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Need to save fs image? false (staleImage=false, haEnabled=false, isRollingUpgrade=false)
2015-01-14 19:46:08,827 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 213
2015-01-14 19:46:09,088 INFO org.apache.hadoop.hdfs.server.namenode.NameCache: initialized with 0 entries 0 lookups
2015-01-14 19:46:09,088 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Finished loading FSImage in 1040 msecs
2015-01-14 19:46:09,863 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: RPC server is binding to localhost:9000
2015-01-14 19:46:09,891 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-01-14 19:46:09,947 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 9000
2015-01-14 19:46:10,206 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Registered FSNamesystemState MBean
2015-01-14 19:46:10,250 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Number of blocks under construction: 0
2015-01-14 19:46:10,250 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Number of blocks under construction: 0
2015-01-14 19:46:10,251 INFO org.apache.hadoop.hdfs.StateChange: STATE* Safe mode ON. 
The reported blocks 0 needs additional 30 blocks to reach the threshold 0.9990 of total blocks 30.
The number of live datanodes 0 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached.
2015-01-14 19:46:10,381 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-01-14 19:46:10,388 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 9000: starting
2015-01-14 19:46:10,395 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: NameNode RPC up at: localhost/127.0.0.1:9000
2015-01-14 19:46:10,406 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Starting services required for active state
2015-01-14 19:46:10,432 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Starting CacheReplicationMonitor with interval 30000 milliseconds
2015-01-14 19:46:10,432 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 4138540 milliseconds
2015-01-14 19:46:10,443 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 10 millisecond(s).
2015-01-14 19:46:28,558 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(127.0.0.1, datanodeUuid=47972a9c-c7e0-400a-ad2b-3a4f63bb22be, infoPort=50075, ipcPort=50020, storageInfo=lv=-56;cid=CID-a0ed2779-c968-4c23-862a-f2f91301194e;nsid=131792437;c=0) storage 47972a9c-c7e0-400a-ad2b-3a4f63bb22be
2015-01-14 19:46:28,575 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2015-01-14 19:46:28,585 INFO org.apache.hadoop.net.NetworkTopology: Adding a new node: /default-rack/127.0.0.1:50010
2015-01-14 19:46:29,439 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2015-01-14 19:46:29,448 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Adding new storage ID DS-bc85abfe-cfe9-4d3b-8deb-497438f4cb62 for DN 127.0.0.1:50010
2015-01-14 19:46:29,875 INFO org.apache.hadoop.hdfs.StateChange: STATE* Safe mode extension entered. 
The reported blocks 29 has reached the threshold 0.9990 of total blocks 30. The number of live datanodes 1 has reached the minimum number 0. In safe mode extension. Safe mode will be turned off automatically in 29 seconds.
2015-01-14 19:46:29,876 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: initializing replication queues
2015-01-14 19:46:29,886 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: BLOCK* processReport: Received first block report from DatanodeStorage[DS-bc85abfe-cfe9-4d3b-8deb-497438f4cb62,DISK,NORMAL] after starting up or becoming active. Its block contents are no longer considered stale
2015-01-14 19:46:29,908 INFO BlockStateChange: BLOCK* processReport: from storage DS-bc85abfe-cfe9-4d3b-8deb-497438f4cb62 node DatanodeRegistration(127.0.0.1, datanodeUuid=47972a9c-c7e0-400a-ad2b-3a4f63bb22be, infoPort=50075, ipcPort=50020, storageInfo=lv=-56;cid=CID-a0ed2779-c968-4c23-862a-f2f91301194e;nsid=131792437;c=0), blocks: 30, hasStaleStorages: false, processing time: 57 msecs
2015-01-14 19:46:29,999 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Total number of blocks            = 30
2015-01-14 19:46:29,999 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of invalid blocks          = 0
2015-01-14 19:46:29,999 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of under-replicated blocks = 0
2015-01-14 19:46:29,999 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of  over-replicated blocks = 0
2015-01-14 19:46:29,999 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of blocks being written    = 0
2015-01-14 19:46:29,999 INFO org.apache.hadoop.hdfs.StateChange: STATE* Replication Queue initialization scan for invalid, over- and under-replicated blocks completed in 113 msec
2015-01-14 19:46:40,433 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-01-14 19:46:40,435 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 2 millisecond(s).
2015-01-14 19:46:49,895 INFO org.apache.hadoop.hdfs.StateChange: STATE* Safe mode ON, in safe mode extension. 
The reported blocks 30 has reached the threshold 0.9990 of total blocks 30. The number of live datanodes 1 has reached the minimum number 0. In safe mode extension. Safe mode will be turned off automatically in 9 seconds.
2015-01-14 19:46:59,896 INFO org.apache.hadoop.hdfs.StateChange: STATE* Leaving safe mode after 52 secs
2015-01-14 19:46:59,896 INFO org.apache.hadoop.hdfs.StateChange: STATE* Safe mode is OFF
2015-01-14 19:46:59,897 INFO org.apache.hadoop.hdfs.StateChange: STATE* Network topology has 1 racks and 1 datanodes
2015-01-14 19:46:59,897 INFO org.apache.hadoop.hdfs.StateChange: STATE* UnderReplicatedBlocks has 0 blocks
2015-01-14 19:47:10,433 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-01-14 19:47:10,435 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 2 millisecond(s).
2015-01-14 19:47:28,215 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Roll Edit Log from 127.0.0.1
2015-01-14 19:47:28,215 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Rolling edit logs
2015-01-14 19:47:28,215 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Ending log segment 213
2015-01-14 19:47:28,216 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 2 Number of transactions batched in Syncs: 0 Number of syncs: 2 SyncTimes(ms): 67 
2015-01-14 19:47:28,236 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 2 Number of transactions batched in Syncs: 0 Number of syncs: 3 SyncTimes(ms): 87 
2015-01-14 19:47:28,238 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /tmp/hadoop-doopy/dfs/name/current/edits_inprogress_0000000000000000213 -> /tmp/hadoop-doopy/dfs/name/current/edits_0000000000000000213-0000000000000000214
2015-01-14 19:47:28,238 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 215
2015-01-14 19:47:30,475 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.04s at 57.14 KB/s
2015-01-14 19:47:30,476 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file fsimage.ckpt_0000000000000000214 size 2983 bytes.
2015-01-14 19:47:30,512 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 211
2015-01-14 19:47:30,512 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/tmp/hadoop-doopy/dfs/name/current/fsimage_0000000000000000000, cpktTxId=0000000000000000000)
2015-01-14 19:47:40,434 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-01-14 19:47:40,436 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 2 millisecond(s).
2015-01-14 19:47:58,830 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Increasing replication from 1 to 10 for /tmp/hadoop-yarn/staging/doopy/.staging/job_1421286406893_0001/job.split
2015-01-14 19:47:58,913 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/doopy/.staging/job_1421286406893_0001/job.split. BP-1611003397-127.0.1.1-1421285133099 blk_1073741856_1032{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-bc85abfe-cfe9-4d3b-8deb-497438f4cb62:NORMAL:127.0.0.1:50010|RBW]]}
2015-01-14 19:47:59,483 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* checkFileProgress: blk_1073741856_1032{blockUCState=COMMITTED, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-bc85abfe-cfe9-4d3b-8deb-497438f4cb62:NORMAL:127.0.0.1:50010|RBW]]} has not reached minimal replication 1
2015-01-14 19:47:59,483 INFO org.apache.hadoop.hdfs.server.namenode.EditLogFileOutputStream: Nothing to flush
2015-01-14 19:47:59,531 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741856_1032{blockUCState=COMMITTED, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-bc85abfe-cfe9-4d3b-8deb-497438f4cb62:NORMAL:127.0.0.1:50010|RBW]]} size 3512
2015-01-14 19:47:59,909 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/doopy/.staging/job_1421286406893_0001/job.split is closed by DFSClient_NONMAPREDUCE_86210742_1
2015-01-14 19:47:59,933 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/doopy/.staging/job_1421286406893_0001/job.splitmetainfo. BP-1611003397-127.0.1.1-1421285133099 blk_1073741857_1033{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-bc85abfe-cfe9-4d3b-8deb-497438f4cb62:NORMAL:127.0.0.1:50010|RBW]]}
2015-01-14 19:47:59,955 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741857_1033{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-bc85abfe-cfe9-4d3b-8deb-497438f4cb62:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-01-14 19:47:59,967 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/doopy/.staging/job_1421286406893_0001/job.splitmetainfo is closed by DFSClient_NONMAPREDUCE_86210742_1
2015-01-14 19:48:00,234 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/doopy/.staging/job_1421286406893_0001/job.xml. BP-1611003397-127.0.1.1-1421285133099 blk_1073741858_1034{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-bc85abfe-cfe9-4d3b-8deb-497438f4cb62:NORMAL:127.0.0.1:50010|RBW]]}
2015-01-14 19:48:00,255 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741858_1034{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-bc85abfe-cfe9-4d3b-8deb-497438f4cb62:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-01-14 19:48:00,267 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/doopy/.staging/job_1421286406893_0001/job.xml is closed by DFSClient_NONMAPREDUCE_86210742_1
2015-01-14 19:48:10,434 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-01-14 19:48:10,436 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 2 millisecond(s).
2015-01-14 19:48:17,126 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/doopy/.staging/job_1421286406893_0001/job_1421286406893_0001_1_conf.xml. BP-1611003397-127.0.1.1-1421285133099 blk_1073741859_1035{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-bc85abfe-cfe9-4d3b-8deb-497438f4cb62:NORMAL:127.0.0.1:50010|RBW]]}
2015-01-14 19:48:17,289 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741859_1035{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-bc85abfe-cfe9-4d3b-8deb-497438f4cb62:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-01-14 19:48:17,300 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/doopy/.staging/job_1421286406893_0001/job_1421286406893_0001_1_conf.xml is closed by DFSClient_NONMAPREDUCE_748411284_1
2015-01-14 19:48:40,435 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-01-14 19:48:40,437 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 3 millisecond(s).
2015-01-14 19:49:16,131 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 35328 milliseconds
2015-01-14 19:49:48,959 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 33135 millisecond(s).
2015-01-14 19:49:49,408 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 33135 milliseconds
2015-01-14 19:49:49,410 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 513 millisecond(s).
2015-01-14 19:50:04,589 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 9849ms
No GCs detected
2015-01-14 19:50:18,897 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-01-14 19:50:18,900 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 3 millisecond(s).
2015-01-14 19:50:48,897 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-01-14 19:50:48,899 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 2 millisecond(s).
2015-01-14 19:50:51,073 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/doopy/.staging/job_1421286406893_0001/job_1421286406893_0001_1.jhist. BP-1611003397-127.0.1.1-1421285133099 blk_1073741860_1036{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-bc85abfe-cfe9-4d3b-8deb-497438f4cb62:NORMAL:127.0.0.1:50010|RBW]]}
2015-01-14 19:50:51,073 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 44 Total time for transactions(ms): 54 Number of transactions batched in Syncs: 0 Number of syncs: 28 SyncTimes(ms): 236 
2015-01-14 19:50:54,844 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* fsync: /tmp/hadoop-yarn/staging/doopy/.staging/job_1421286406893_0001/job_1421286406893_0001_1.jhist for DFSClient_NONMAPREDUCE_748411284_1
2015-01-14 19:51:18,898 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-01-14 19:51:18,900 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 2 millisecond(s).
2015-01-14 19:51:50,938 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30182 milliseconds
2015-01-14 19:52:05,443 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1854ms
No GCs detected
2015-01-14 19:52:09,453 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 3509ms
No GCs detected
2015-01-14 19:52:11,461 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1506ms
No GCs detected
2015-01-14 19:52:13,019 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1058ms
No GCs detected
2015-01-14 19:52:15,781 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 26701 millisecond(s).
2015-01-14 19:52:20,269 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 31189 milliseconds
2015-01-14 19:52:20,272 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 2 millisecond(s).
2015-01-14 19:52:25,149 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1528ms
No GCs detected
2015-01-14 19:52:42,511 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1716ms
No GCs detected
2015-01-14 19:52:50,270 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-01-14 19:52:50,273 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 2 millisecond(s).
2015-01-14 19:53:20,271 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-01-14 19:53:20,274 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 2 millisecond(s).
2015-01-14 19:53:56,319 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 35959 milliseconds
2015-01-14 19:54:15,004 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 4776ms
No GCs detected
2015-01-14 19:54:17,274 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 21044 millisecond(s).
2015-01-14 19:54:21,650 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 2832ms
No GCs detected
2015-01-14 19:54:23,914 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1764ms
No GCs detected
2015-01-14 19:54:27,352 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 31121 milliseconds
2015-01-14 19:54:27,354 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 3 millisecond(s).
2015-01-14 19:54:27,354 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1403ms
No GCs detected
2015-01-14 19:54:57,351 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-01-14 19:54:57,475 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 124 millisecond(s).
2015-01-14 19:55:27,352 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-01-14 19:55:27,354 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 2 millisecond(s).
2015-01-14 19:55:57,937 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30290 milliseconds
2015-01-14 19:56:12,860 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 15218 millisecond(s).
2015-01-14 19:56:19,889 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 5831ms
No GCs detected
2015-01-14 19:56:33,577 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 35935 milliseconds
2015-01-14 19:56:33,578 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 5687ms
No GCs detected
2015-01-14 19:56:37,027 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 3450 millisecond(s).
2015-01-14 19:56:39,317 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1791ms
No GCs detected
2015-01-14 19:57:03,577 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-01-14 19:57:03,622 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 45 millisecond(s).
2015-01-14 19:57:33,578 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-01-14 19:57:33,589 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 12 millisecond(s).
2015-01-14 19:58:03,577 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-01-14 19:58:03,579 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 2 millisecond(s).
2015-01-14 19:58:34,094 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30371 milliseconds
2015-01-14 19:58:51,590 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 4609ms
No GCs detected
2015-01-14 19:58:56,263 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 3171ms
No GCs detected
2015-01-14 19:58:56,818 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 22870 millisecond(s).
2015-01-14 19:58:58,883 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1119ms
No GCs detected
2015-01-14 19:59:04,034 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-01-14 19:59:04,036 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 88 millisecond(s).
2015-01-14 19:59:34,002 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30053 milliseconds
2015-01-14 19:59:34,535 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 534 millisecond(s).
2015-01-14 19:59:34,535 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1013ms
No GCs detected
2015-01-14 20:00:04,001 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-01-14 20:00:04,031 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 2 millisecond(s).
2015-01-14 20:00:34,029 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30028 milliseconds
2015-01-14 20:00:34,031 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 2 millisecond(s).
2015-01-14 20:01:04,094 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30065 milliseconds
2015-01-14 20:01:07,720 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 3626 millisecond(s).
2015-01-14 20:01:19,416 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 6698ms
No GCs detected
2015-01-14 20:01:34,110 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30016 milliseconds
2015-01-14 20:01:34,256 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 145 millisecond(s).
2015-01-14 20:01:34,111 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1190ms
No GCs detected
2015-01-14 20:01:49,783 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1399ms
No GCs detected
2015-01-14 20:02:26,033 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 51922 milliseconds
2015-01-14 20:02:26,035 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 3 millisecond(s).
2015-01-14 20:02:26,388 WARN org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 30913ms
GC pool 'Copy' had collection(s): count=1 time=31123ms
2015-01-14 20:02:27,242 WARN org.apache.hadoop.security.Groups: Potential performance problem: getGroups(user=doopy) took 32330 milliseconds.
2015-01-14 20:02:47,116 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 48 Total time for transactions(ms): 95 Number of transactions batched in Syncs: 0 Number of syncs: 30 SyncTimes(ms): 500 
2015-01-14 20:02:47,775 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /user/doopy/grep-temp-497817351/_temporary/1/_temporary/attempt_1421286406893_0001_r_000000_0/part-r-00000. BP-1611003397-127.0.1.1-1421285133099 blk_1073741861_1037{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-bc85abfe-cfe9-4d3b-8deb-497438f4cb62:NORMAL:127.0.0.1:50010|RBW]]}
2015-01-14 20:02:49,125 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741861_1037{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-bc85abfe-cfe9-4d3b-8deb-497438f4cb62:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-01-14 20:02:49,167 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/doopy/grep-temp-497817351/_temporary/1/_temporary/attempt_1421286406893_0001_r_000000_0/part-r-00000 is closed by DFSClient_attempt_1421286406893_0001_r_000000_0_-836744152_1
2015-01-14 20:02:51,042 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/doopy/.staging/job_1421286406893_0001/COMMIT_STARTED is closed by DFSClient_NONMAPREDUCE_748411284_1
2015-01-14 20:02:51,600 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/doopy/grep-temp-497817351/_SUCCESS is closed by DFSClient_NONMAPREDUCE_748411284_1
2015-01-14 20:02:51,616 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/doopy/.staging/job_1421286406893_0001/COMMIT_SUCCESS is closed by DFSClient_NONMAPREDUCE_748411284_1
2015-01-14 20:02:56,032 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-01-14 20:02:56,034 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 2 millisecond(s).
2015-01-14 20:02:58,683 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741856_1032 127.0.0.1:50010 
2015-01-14 20:02:58,684 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741857_1033 127.0.0.1:50010 
2015-01-14 20:02:58,684 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741858_1034 127.0.0.1:50010 
2015-01-14 20:02:58,684 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741859_1035 127.0.0.1:50010 
2015-01-14 20:02:59,167 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* BlockManager: ask 127.0.0.1:50010 to delete [blk_1073741856_1032, blk_1073741857_1033, blk_1073741858_1034, blk_1073741859_1035]
2015-01-14 20:03:05,912 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Increasing replication from 1 to 10 for /tmp/hadoop-yarn/staging/doopy/.staging/job_1421286406893_0002/job.split
2015-01-14 20:03:05,996 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/doopy/.staging/job_1421286406893_0002/job.split. BP-1611003397-127.0.1.1-1421285133099 blk_1073741862_1038{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-bc85abfe-cfe9-4d3b-8deb-497438f4cb62:NORMAL:127.0.0.1:50010|RBW]]}
2015-01-14 20:03:06,072 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741862_1038{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-bc85abfe-cfe9-4d3b-8deb-497438f4cb62:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-01-14 20:03:06,101 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/doopy/.staging/job_1421286406893_0002/job.split is closed by DFSClient_NONMAPREDUCE_86210742_1
2015-01-14 20:03:06,121 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/doopy/.staging/job_1421286406893_0002/job.splitmetainfo. BP-1611003397-127.0.1.1-1421285133099 blk_1073741863_1039{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-bc85abfe-cfe9-4d3b-8deb-497438f4cb62:NORMAL:127.0.0.1:50010|RBW]]}
2015-01-14 20:03:06,136 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741863_1039{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-bc85abfe-cfe9-4d3b-8deb-497438f4cb62:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-01-14 20:03:06,143 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/doopy/.staging/job_1421286406893_0002/job.splitmetainfo is closed by DFSClient_NONMAPREDUCE_86210742_1
2015-01-14 20:03:06,348 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/doopy/.staging/job_1421286406893_0002/job.xml. BP-1611003397-127.0.1.1-1421285133099 blk_1073741864_1040{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-bc85abfe-cfe9-4d3b-8deb-497438f4cb62:NORMAL:127.0.0.1:50010|RBW]]}
2015-01-14 20:03:06,459 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741864_1040{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-bc85abfe-cfe9-4d3b-8deb-497438f4cb62:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-01-14 20:03:06,484 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/doopy/.staging/job_1421286406893_0002/job.xml is closed by DFSClient_NONMAPREDUCE_86210742_1
2015-01-14 20:03:26,033 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-01-14 20:03:26,035 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 3 millisecond(s).
2015-01-14 20:03:56,032 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-01-14 20:03:56,034 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 2 millisecond(s).
2015-01-14 20:04:26,033 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-01-14 20:04:26,035 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 3 millisecond(s).
2015-01-14 20:04:56,032 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-01-14 20:04:56,034 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 2 millisecond(s).
2015-01-14 20:05:26,033 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-01-14 20:05:26,035 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 3 millisecond(s).
2015-01-14 20:05:54,380 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 84 Total time for transactions(ms): 104 Number of transactions batched in Syncs: 0 Number of syncs: 58 SyncTimes(ms): 1119 
2015-01-14 20:05:54,874 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Increasing replication from 1 to 10 for /tmp/hadoop-yarn/staging/doopy/.staging/job_1421286406893_0003/job.split
2015-01-14 20:05:54,955 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/doopy/.staging/job_1421286406893_0003/job.split. BP-1611003397-127.0.1.1-1421285133099 blk_1073741865_1041{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-bc85abfe-cfe9-4d3b-8deb-497438f4cb62:NORMAL:127.0.0.1:50010|RBW]]}
2015-01-14 20:05:55,227 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741865_1041{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-bc85abfe-cfe9-4d3b-8deb-497438f4cb62:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-01-14 20:05:55,277 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/doopy/.staging/job_1421286406893_0003/job.split is closed by DFSClient_NONMAPREDUCE_1385649367_1
2015-01-14 20:05:55,335 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/doopy/.staging/job_1421286406893_0003/job.splitmetainfo. BP-1611003397-127.0.1.1-1421285133099 blk_1073741866_1042{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-bc85abfe-cfe9-4d3b-8deb-497438f4cb62:NORMAL:127.0.0.1:50010|RBW]]}
2015-01-14 20:05:55,354 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741866_1042{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-bc85abfe-cfe9-4d3b-8deb-497438f4cb62:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-01-14 20:05:55,360 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/doopy/.staging/job_1421286406893_0003/job.splitmetainfo is closed by DFSClient_NONMAPREDUCE_1385649367_1
2015-01-14 20:05:55,857 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/doopy/.staging/job_1421286406893_0003/job.xml. BP-1611003397-127.0.1.1-1421285133099 blk_1073741867_1043{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-bc85abfe-cfe9-4d3b-8deb-497438f4cb62:NORMAL:127.0.0.1:50010|RBW]]}
2015-01-14 20:05:55,891 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741867_1043{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-bc85abfe-cfe9-4d3b-8deb-497438f4cb62:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-01-14 20:05:55,902 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/doopy/.staging/job_1421286406893_0003/job.xml is closed by DFSClient_NONMAPREDUCE_1385649367_1
2015-01-14 20:05:56,032 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-01-14 20:05:56,034 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 2 millisecond(s).
2015-01-14 20:06:17,780 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Increasing replication from 1 to 10 for /tmp/hadoop-yarn/staging/doopy/.staging/job_1421286406893_0004/job.split
2015-01-14 20:06:17,835 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/doopy/.staging/job_1421286406893_0004/job.split. BP-1611003397-127.0.1.1-1421285133099 blk_1073741868_1044{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-bc85abfe-cfe9-4d3b-8deb-497438f4cb62:NORMAL:127.0.0.1:50010|RBW]]}
2015-01-14 20:06:18,077 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741868_1044{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-bc85abfe-cfe9-4d3b-8deb-497438f4cb62:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-01-14 20:06:18,099 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/doopy/.staging/job_1421286406893_0004/job.split is closed by DFSClient_NONMAPREDUCE_2145073534_1
2015-01-14 20:06:18,133 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/doopy/.staging/job_1421286406893_0004/job.splitmetainfo. BP-1611003397-127.0.1.1-1421285133099 blk_1073741869_1045{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-bc85abfe-cfe9-4d3b-8deb-497438f4cb62:NORMAL:127.0.0.1:50010|RBW]]}
2015-01-14 20:06:18,151 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741869_1045{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-bc85abfe-cfe9-4d3b-8deb-497438f4cb62:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-01-14 20:06:18,166 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/doopy/.staging/job_1421286406893_0004/job.splitmetainfo is closed by DFSClient_NONMAPREDUCE_2145073534_1
2015-01-14 20:06:18,487 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/doopy/.staging/job_1421286406893_0004/job.xml. BP-1611003397-127.0.1.1-1421285133099 blk_1073741870_1046{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-bc85abfe-cfe9-4d3b-8deb-497438f4cb62:NORMAL:127.0.0.1:50010|RBW]]}
2015-01-14 20:06:18,552 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741870_1046{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-bc85abfe-cfe9-4d3b-8deb-497438f4cb62:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-01-14 20:06:18,574 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/doopy/.staging/job_1421286406893_0004/job.xml is closed by DFSClient_NONMAPREDUCE_2145073534_1
2015-01-14 20:06:26,033 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-01-14 20:06:26,035 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 3 millisecond(s).
2015-01-14 20:06:56,032 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-01-14 20:06:56,034 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 2 millisecond(s).
2015-01-14 20:07:26,033 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-01-14 20:07:26,035 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 3 millisecond(s).
2015-01-14 20:07:56,032 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-01-14 20:07:56,034 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 2 millisecond(s).
2015-01-14 20:08:26,033 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-01-14 20:08:26,035 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 3 millisecond(s).
2015-01-14 20:08:56,032 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-01-14 20:08:56,034 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 2 millisecond(s).
2015-01-14 20:09:26,032 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-01-14 20:09:26,035 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 3 millisecond(s).
2015-01-14 20:09:56,032 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-01-14 20:09:56,034 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 2 millisecond(s).
2015-01-14 20:10:26,032 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-01-14 20:10:26,035 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 2 millisecond(s).
2015-01-14 20:10:56,033 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-01-14 20:10:56,035 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 2 millisecond(s).
2015-01-14 20:11:26,033 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-01-14 20:11:26,036 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 2 millisecond(s).
2015-01-14 20:11:56,034 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-01-14 20:11:56,036 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 2 millisecond(s).
2015-01-14 20:12:26,034 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-01-14 20:12:26,037 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 2 millisecond(s).
2015-01-14 20:12:56,035 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-01-14 20:12:56,037 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 2 millisecond(s).
2015-01-14 20:13:26,035 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-01-14 20:13:26,037 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 2 millisecond(s).
2015-01-14 20:13:41,074 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 128 Total time for transactions(ms): 115 Number of transactions batched in Syncs: 0 Number of syncs: 88 SyncTimes(ms): 1523 
2015-01-14 20:13:41,633 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/doopy/.staging/job_1421286406893_0002/job_1421286406893_0002_1_conf.xml. BP-1611003397-127.0.1.1-1421285133099 blk_1073741871_1047{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-bc85abfe-cfe9-4d3b-8deb-497438f4cb62:NORMAL:127.0.0.1:50010|RBW]]}
2015-01-14 20:13:41,801 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741871_1047{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-bc85abfe-cfe9-4d3b-8deb-497438f4cb62:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-01-14 20:13:41,816 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/doopy/.staging/job_1421286406893_0002/job_1421286406893_0002_1_conf.xml is closed by DFSClient_NONMAPREDUCE_541068657_1
2015-01-14 20:13:49,319 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/doopy/.staging/job_1421286406893_0002/job_1421286406893_0002_1.jhist. BP-1611003397-127.0.1.1-1421285133099 blk_1073741872_1048{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-bc85abfe-cfe9-4d3b-8deb-497438f4cb62:NORMAL:127.0.0.1:50010|RBW]]}
2015-01-14 20:13:49,380 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* fsync: /tmp/hadoop-yarn/staging/doopy/.staging/job_1421286406893_0002/job_1421286406893_0002_1.jhist for DFSClient_NONMAPREDUCE_541068657_1
2015-01-14 20:13:56,036 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-01-14 20:13:56,038 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 2 millisecond(s).
2015-01-14 20:13:57,466 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /user/doopy/output2/_temporary/1/_temporary/attempt_1421286406893_0002_r_000000_0/part-r-00000. BP-1611003397-127.0.1.1-1421285133099 blk_1073741873_1049{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-bc85abfe-cfe9-4d3b-8deb-497438f4cb62:NORMAL:127.0.0.1:50010|RBW]]}
2015-01-14 20:13:57,781 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741873_1049{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-bc85abfe-cfe9-4d3b-8deb-497438f4cb62:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-01-14 20:13:57,808 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/doopy/output2/_temporary/1/_temporary/attempt_1421286406893_0002_r_000000_0/part-r-00000 is closed by DFSClient_attempt_1421286406893_0002_r_000000_0_-879256423_1
2015-01-14 20:13:58,100 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/doopy/.staging/job_1421286406893_0002/COMMIT_STARTED is closed by DFSClient_NONMAPREDUCE_541068657_1
2015-01-14 20:13:58,225 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/doopy/output2/_SUCCESS is closed by DFSClient_NONMAPREDUCE_541068657_1
2015-01-14 20:13:58,242 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/doopy/.staging/job_1421286406893_0002/COMMIT_SUCCESS is closed by DFSClient_NONMAPREDUCE_541068657_1
2015-01-14 20:13:58,290 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741872_1048{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-bc85abfe-cfe9-4d3b-8deb-497438f4cb62:NORMAL:127.0.0.1:50010|RBW]]} size 12964
2015-01-14 20:13:58,325 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/doopy/.staging/job_1421286406893_0002/job_1421286406893_0002_1.jhist is closed by DFSClient_NONMAPREDUCE_541068657_1
2015-01-14 20:13:58,339 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/doopy/job_1421286406893_0002.summary_tmp. BP-1611003397-127.0.1.1-1421285133099 blk_1073741874_1050{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-bc85abfe-cfe9-4d3b-8deb-497438f4cb62:NORMAL:127.0.0.1:50010|RBW]]}
2015-01-14 20:13:58,385 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741874_1050{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-bc85abfe-cfe9-4d3b-8deb-497438f4cb62:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-01-14 20:13:58,391 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/doopy/job_1421286406893_0002.summary_tmp is closed by DFSClient_NONMAPREDUCE_541068657_1
2015-01-14 20:13:58,493 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/doopy/job_1421286406893_0002-1421287386734-doopy-grep%2Dsort-1421288038243-1-1-SUCCEEDED-default-1421288021038.jhist_tmp. BP-1611003397-127.0.1.1-1421285133099 blk_1073741875_1051{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-bc85abfe-cfe9-4d3b-8deb-497438f4cb62:NORMAL:127.0.0.1:50010|RBW]]}
2015-01-14 20:13:58,511 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741875_1051{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-bc85abfe-cfe9-4d3b-8deb-497438f4cb62:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-01-14 20:13:58,516 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/doopy/job_1421286406893_0002-1421287386734-doopy-grep%2Dsort-1421288038243-1-1-SUCCEEDED-default-1421288021038.jhist_tmp is closed by DFSClient_NONMAPREDUCE_541068657_1
2015-01-14 20:13:58,596 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/history/done_intermediate/doopy/job_1421286406893_0002_conf.xml_tmp. BP-1611003397-127.0.1.1-1421285133099 blk_1073741876_1052{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-bc85abfe-cfe9-4d3b-8deb-497438f4cb62:NORMAL:127.0.0.1:50010|RBW]]}
2015-01-14 20:13:58,613 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741876_1052{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-bc85abfe-cfe9-4d3b-8deb-497438f4cb62:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-01-14 20:13:58,625 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/history/done_intermediate/doopy/job_1421286406893_0002_conf.xml_tmp is closed by DFSClient_NONMAPREDUCE_541068657_1
2015-01-14 20:13:59,691 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741862_1038 127.0.0.1:50010 
2015-01-14 20:13:59,691 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741863_1039 127.0.0.1:50010 
2015-01-14 20:13:59,691 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741864_1040 127.0.0.1:50010 
2015-01-14 20:13:59,691 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741872_1048 127.0.0.1:50010 
2015-01-14 20:13:59,692 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741871_1047 127.0.0.1:50010 
2015-01-14 20:14:02,351 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* BlockManager: ask 127.0.0.1:50010 to delete [blk_1073741872_1048, blk_1073741862_1038, blk_1073741863_1039, blk_1073741864_1040, blk_1073741871_1047]
2015-01-14 20:14:13,707 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocateBlock: /tmp/hadoop-yarn/staging/doopy/.staging/job_1421286406893_0003/job_1421286406893_0003_1_conf.xml. BP-1611003397-127.0.1.1-1421285133099 blk_1073741877_1053{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-bc85abfe-cfe9-4d3b-8deb-497438f4cb62:NORMAL:127.0.0.1:50010|RBW]]}
2015-01-14 20:14:13,918 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:50010 is added to blk_1073741877_1053{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[[DISK]DS-bc85abfe-cfe9-4d3b-8deb-497438f4cb62:NORMAL:127.0.0.1:50010|RBW]]} size 0
2015-01-14 20:14:13,951 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-yarn/staging/doopy/.staging/job_1421286406893_0003/job_1421286406893_0003_1_conf.xml is closed by DFSClient_NONMAPREDUCE_-2135711393_1
2015-01-14 20:14:26,036 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-01-14 20:14:26,039 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 2 millisecond(s).
2015-01-14 20:15:00,439 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 33369 milliseconds
2015-01-14 20:15:17,912 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 18507 millisecond(s).
2015-01-14 20:15:29,405 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-01-14 20:15:30,648 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 1243 millisecond(s).
2015-01-14 20:15:34,629 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1655ms
No GCs detected
2015-01-14 20:15:39,352 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 2447ms
No GCs detected
2015-01-14 20:15:42,561 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 2207ms
No GCs detected
2015-01-14 20:16:00,284 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30879 milliseconds
2015-01-14 20:16:00,286 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 2 millisecond(s).
2015-01-14 20:16:30,284 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-01-14 20:16:30,286 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 2 millisecond(s).
2015-01-14 20:17:00,285 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30001 milliseconds
2015-01-14 20:17:00,287 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 2 millisecond(s).
2015-01-14 20:17:30,285 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-01-14 20:17:30,287 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 2 millisecond(s).
2015-01-14 20:18:00,286 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-01-14 20:18:00,288 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 3 millisecond(s).
2015-01-14 20:18:30,285 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after 30000 milliseconds
2015-01-14 20:18:30,287 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned 0 directive(s) and 0 block(s) in 2 millisecond(s).
2015-01-14 20:18:49,688 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: RECEIVED SIGNAL 15: SIGTERM
2015-01-14 20:18:50,529 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down NameNode at barbie/127.0.1.1
************************************************************/
